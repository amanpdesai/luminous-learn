{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5261ca69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-genai in /Users/aj/Downloads/Code/hackathons/luminous/.venv/lib/python3.13/site-packages (1.11.0)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /Users/aj/Downloads/Code/hackathons/luminous/.venv/lib/python3.13/site-packages (from google-genai) (4.9.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /Users/aj/Downloads/Code/hackathons/luminous/.venv/lib/python3.13/site-packages (from google-genai) (2.39.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /Users/aj/Downloads/Code/hackathons/luminous/.venv/lib/python3.13/site-packages (from google-genai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /Users/aj/Downloads/Code/hackathons/luminous/.venv/lib/python3.13/site-packages (from google-genai) (2.11.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /Users/aj/Downloads/Code/hackathons/luminous/.venv/lib/python3.13/site-packages (from google-genai) (2.32.3)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /Users/aj/Downloads/Code/hackathons/luminous/.venv/lib/python3.13/site-packages (from google-genai) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /Users/aj/Downloads/Code/hackathons/luminous/.venv/lib/python3.13/site-packages (from google-genai) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/aj/Downloads/Code/hackathons/luminous/.venv/lib/python3.13/site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/aj/Downloads/Code/hackathons/luminous/.venv/lib/python3.13/site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/aj/Downloads/Code/hackathons/luminous/.venv/lib/python3.13/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/aj/Downloads/Code/hackathons/luminous/.venv/lib/python3.13/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/aj/Downloads/Code/hackathons/luminous/.venv/lib/python3.13/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in /Users/aj/Downloads/Code/hackathons/luminous/.venv/lib/python3.13/site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/aj/Downloads/Code/hackathons/luminous/.venv/lib/python3.13/site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/aj/Downloads/Code/hackathons/luminous/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/aj/Downloads/Code/hackathons/luminous/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /Users/aj/Downloads/Code/hackathons/luminous/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/aj/Downloads/Code/hackathons/luminous/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/aj/Downloads/Code/hackathons/luminous/.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/aj/Downloads/Code/hackathons/luminous/.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.4.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /Users/aj/Downloads/Code/hackathons/luminous/.venv/lib/python3.13/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba5d24ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dotenv\n",
      "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Requirement already satisfied: python-dotenv in /Users/aj/Downloads/Code/hackathons/luminous/.venv/lib/python3.13/site-packages (from dotenv) (1.1.0)\n",
      "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Installing collected packages: dotenv\n",
      "Successfully installed dotenv-0.9.9\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b03cb29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "from google import genai\n",
    "import os\n",
    "import requests \n",
    "from dotenv import load_dotenv\n",
    "import threading \n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "306a425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in os.environ:\n",
    "        if key.startswith(\"GOOGLE_\"):\n",
    "                print(key, os.environ[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7db05e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "google_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "client = genai.Client(api_key=google_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42642955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class Lesson(BaseModel):\n",
    "    lesson: str\n",
    "    duration_minutes: int\n",
    "    topics: List[str]\n",
    "\n",
    "\n",
    "class Unit(BaseModel):\n",
    "    Unit_number: int\n",
    "    title: str\n",
    "    unit_description: str\n",
    "    learning_objectives: List[str]\n",
    "    lesson_outline: List[Lesson]\n",
    "    assessment: str\n",
    "\n",
    "\n",
    "class FinalAssessment(BaseModel):\n",
    "    final_exam_description: str\n",
    "\n",
    "\n",
    "class Course(BaseModel):\n",
    "    title: str\n",
    "    description: str\n",
    "    estimated_duration_hours: int\n",
    "    prerequisites: List[str]\n",
    "    units: List[Unit]\n",
    "    final_assessment: FinalAssessment\n",
    "\n",
    "\n",
    "class Syllabus(BaseModel):\n",
    "    course: Course\n",
    "\n",
    "class LessonTopic(BaseModel):\n",
    "    title: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10a93e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_syllabus(topic: str, difficulty: str):\n",
    "#     response = client.models.generate_content(\n",
    "#         model=\"gemini-2.0-flash\",\n",
    "#         contents=[\n",
    "#             f\"Generate a complete syllabus for a {difficulty} {topic} class.\",\n",
    "#             \"Include lesson titles, and for each, provide Lesson Description,Learning Objectives, Lesson Outline, and Assessment.\",\n",
    "#             \"End with a Final Exam section detailing topics that will be tested.\",\n",
    "#             \"Return the output as structured JSON only.\",\n",
    "#         ],\n",
    "#         config={\n",
    "#             'response_mime_type': 'application/json',\n",
    "#             'response_schema': Syllabus,\n",
    "#         },\n",
    "#     )\n",
    "#     return response.text\n",
    "\n",
    "'''uses the fields on the website'''\n",
    "\n",
    "def get_syllabus(topic: str, difficulty: str, depth: str) -> str:\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=[\n",
    "            f\"Generate a complete syllabus for a {difficulty} class of depth {depth}.\",\n",
    "            f\"The subject is: {topic}\",\n",
    "            \"Include lesson titles, and for each, provide Lesson Description,Learning Objectives, Lesson Outline, and Assessment.\",\n",
    "            \"End with a Final Exam section detailing topics that will be tested.\",\n",
    "            \"Return the output as structured JSON only.\",\n",
    "        ],\n",
    "        config={\n",
    "            'response_mime_type': 'application/json',\n",
    "            'response_schema': Syllabus,\n",
    "        },\n",
    "    )\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ad5a843",
   "metadata": {},
   "outputs": [],
   "source": [
    "syllabus = get_syllabus('machine learning','easy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51d85228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"course\": {\n",
      "    \"title\": \"Introduction to Machine Learning\",\n",
      "    \"description\": \"This course provides a gentle introduction to the fundamental concepts and techniques of machine learning. Students will learn about supervised and unsupervised learning algorithms, model evaluation, and practical considerations for applying machine learning in real-world scenarios. No prior machine learning experience is required.\",\n",
      "    \"estimated_duration_hours\": 45,\n",
      "    \"prerequisites\": [\n",
      "      \"Basic programming skills (e.g., Python)\",\n",
      "      \"Basic understanding of linear algebra and statistics\"\n",
      "    ],\n",
      "    \"units\": [\n",
      "      {\n",
      "        \"Unit_number\": 1,\n",
      "        \"title\": \"Introduction to Machine Learning\",\n",
      "        \"unit_description\": \"This unit introduces the core concepts of machine learning, its different types, and the machine learning workflow.\",\n",
      "        \"learning_objectives\": [\n",
      "          \"Define machine learning and its applications.\",\n",
      "          \"Distinguish between supervised, unsupervised, and reinforcement learning.\",\n",
      "          \"Understand the machine learning workflow.\",\n",
      "          \"Explain the concepts of features, labels, and models.\"\n",
      "        ],\n",
      "        \"lesson_outline\": [\n",
      "          {\n",
      "            \"lesson\": \"What is Machine Learning?\",\n",
      "            \"duration_minutes\": 60,\n",
      "            \"topics\": [\n",
      "              \"Definition of Machine Learning\",\n",
      "              \"Applications of Machine Learning\",\n",
      "              \"Types of Machine Learning: Supervised, Unsupervised, Reinforcement Learning\"\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"lesson\": \"The Machine Learning Workflow\",\n",
      "            \"duration_minutes\": 75,\n",
      "            \"topics\": [\n",
      "              \"Data Collection and Preparation\",\n",
      "              \"Model Selection\",\n",
      "              \"Training and Evaluation\",\n",
      "              \"Deployment and Monitoring\"\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"lesson\": \"Features, Labels, and Models\",\n",
      "            \"duration_minutes\": 45,\n",
      "            \"topics\": [\n",
      "              \"Understanding Features and Labels\",\n",
      "              \"Introduction to Different Types of Models\"\n",
      "            ]\n",
      "          }\n",
      "        ],\n",
      "        \"assessment\": \"Short quiz on the definitions and types of machine learning.\"\n",
      "      },\n",
      "      {\n",
      "        \"Unit_number\": 2,\n",
      "        \"title\": \"Supervised Learning: Regression\",\n",
      "        \"unit_description\": \"This unit covers regression algorithms, focusing on linear regression and polynomial regression. Students will learn how to build, train, and evaluate regression models.\",\n",
      "        \"learning_objectives\": [\n",
      "          \"Explain the concept of regression.\",\n",
      "          \"Implement linear regression using Python.\",\n",
      "          \"Evaluate regression models using metrics like Mean Squared Error (MSE).\",\n",
      "          \"Understand the concept of overfitting and underfitting.\"\n",
      "        ],\n",
      "        \"lesson_outline\": [\n",
      "          {\n",
      "            \"lesson\": \"Introduction to Regression\",\n",
      "            \"duration_minutes\": 60,\n",
      "            \"topics\": [\n",
      "              \"What is Regression?\",\n",
      "              \"Types of Regression: Linear, Polynomial\",\n",
      "              \"Applications of Regression\"\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"lesson\": \"Linear Regression\",\n",
      "            \"duration_minutes\": 90,\n",
      "            \"topics\": [\n",
      "              \"Simple Linear Regression\",\n",
      "              \"Multiple Linear Regression\",\n",
      "              \"Implementation with Python (scikit-learn)\"\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"lesson\": \"Model Evaluation and Overfitting\",\n",
      "            \"duration_minutes\": 60,\n",
      "            \"topics\": [\n",
      "              \"Mean Squared Error (MSE)\",\n",
      "              \"R-squared\",\n",
      "              \"Overfitting and Underfitting\",\n",
      "              \"Regularization (brief introduction)\"\n",
      "            ]\n",
      "          }\n",
      "        ],\n",
      "        \"assessment\": \"Programming assignment: Build and evaluate a linear regression model on a given dataset.\"\n",
      "      },\n",
      "      {\n",
      "        \"Unit_number\": 3,\n",
      "        \"title\": \"Supervised Learning: Classification\",\n",
      "        \"unit_description\": \"This unit introduces classification algorithms, including logistic regression and support vector machines. Students will learn how to classify data into different categories.\",\n",
      "        \"learning_objectives\": [\n",
      "          \"Explain the concept of classification.\",\n",
      "          \"Implement logistic regression using Python.\",\n",
      "          \"Understand the concept of Support Vector Machines (SVM).\",\n",
      "          \"Evaluate classification models using metrics like accuracy, precision, and recall.\"\n",
      "        ],\n",
      "        \"lesson_outline\": [\n",
      "          {\n",
      "            \"lesson\": \"Introduction to Classification\",\n",
      "            \"duration_minutes\": 60,\n",
      "            \"topics\": [\n",
      "              \"What is Classification?\",\n",
      "              \"Types of Classification Problems\",\n",
      "              \"Applications of Classification\"\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"lesson\": \"Logistic Regression\",\n",
      "            \"duration_minutes\": 90,\n",
      "            \"topics\": [\n",
      "              \"Sigmoid Function\",\n",
      "              \"Cost Function for Logistic Regression\",\n",
      "              \"Implementation with Python (scikit-learn)\"\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"lesson\": \"Support Vector Machines (SVM)\",\n",
      "            \"duration_minutes\": 75,\n",
      "            \"topics\": [\n",
      "              \"Introduction to SVM\",\n",
      "              \"Kernels\",\n",
      "              \"Implementation with Python (scikit-learn)\"\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"lesson\": \"Model Evaluation\",\n",
      "            \"duration_minutes\": 45,\n",
      "            \"topics\": [\n",
      "              \"Accuracy, Precision, Recall, F1-Score\",\n",
      "              \"Confusion Matrix\"\n",
      "            ]\n",
      "          }\n",
      "        ],\n",
      "        \"assessment\": \"Programming assignment: Build and evaluate a classification model (Logistic Regression or SVM) on a given dataset.\"\n",
      "      },\n",
      "      {\n",
      "        \"Unit_number\": 4,\n",
      "        \"title\": \"Unsupervised Learning: Clustering\",\n",
      "        \"unit_description\": \"This unit introduces unsupervised learning, focusing on clustering algorithms like K-Means. Students will learn how to group data points into clusters.\",\n",
      "        \"learning_objectives\": [\n",
      "          \"Explain the concept of unsupervised learning.\",\n",
      "          \"Implement K-Means clustering using Python.\",\n",
      "          \"Evaluate clustering models using metrics like silhouette score.\",\n",
      "          \"Understand the applications of clustering.\"\n",
      "        ],\n",
      "        \"lesson_outline\": [\n",
      "          {\n",
      "            \"lesson\": \"Introduction to Unsupervised Learning\",\n",
      "            \"duration_minutes\": 45,\n",
      "            \"topics\": [\n",
      "              \"What is Unsupervised Learning?\",\n",
      "              \"Types of Unsupervised Learning: Clustering, Dimensionality Reduction\",\n",
      "              \"Applications of Unsupervised Learning\"\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"lesson\": \"K-Means Clustering\",\n",
      "            \"duration_minutes\": 90,\n",
      "            \"topics\": [\n",
      "              \"K-Means Algorithm\",\n",
      "              \"Choosing the Number of Clusters (Elbow Method)\",\n",
      "              \"Implementation with Python (scikit-learn)\"\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"lesson\": \"Model Evaluation\",\n",
      "            \"duration_minutes\": 60,\n",
      "            \"topics\": [\n",
      "              \"Silhouette Score\",\n",
      "              \"Visualizing Clusters\"\n",
      "            ]\n",
      "          }\n",
      "        ],\n",
      "        \"assessment\": \"Programming assignment: Apply K-Means clustering to a given dataset and evaluate the results.\"\n",
      "      },\n",
      "      {\n",
      "        \"Unit_number\": 5,\n",
      "        \"title\": \"Model Evaluation and Selection\",\n",
      "        \"unit_description\": \"This unit focuses on advanced model evaluation techniques and how to select the best model for a given problem.\",\n",
      "        \"learning_objectives\": [\n",
      "          \"Understand cross-validation techniques.\",\n",
      "          \"Apply grid search for hyperparameter tuning.\",\n",
      "          \"Choose appropriate evaluation metrics for different types of problems.\"\n",
      "        ],\n",
      "        \"lesson_outline\": [\n",
      "          {\n",
      "            \"lesson\": \"Cross-Validation\",\n",
      "            \"duration_minutes\": 75,\n",
      "            \"topics\": [\n",
      "              \"K-Fold Cross-Validation\",\n",
      "              \"Stratified K-Fold Cross-Validation\"\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"lesson\": \"Hyperparameter Tuning\",\n",
      "            \"duration_minutes\": 90,\n",
      "            \"topics\": [\n",
      "              \"Grid Search\",\n",
      "              \"Randomized Search\"\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"lesson\": \"Choosing Evaluation Metrics\",\n",
      "            \"duration_minutes\": 45,\n",
      "            \"topics\": [\n",
      "              \"Selecting Appropriate Metrics Based on Problem Type\"\n",
      "            ]\n",
      "          }\n",
      "        ],\n",
      "        \"assessment\": \"Programming assignment: Implement cross-validation and grid search to optimize a model's performance.\"\n",
      "      }\n",
      "    ],\n",
      "    \"final_assessment\": {\n",
      "      \"final_exam_description\": \"The final exam will cover all topics discussed in the course, including supervised learning (regression and classification), unsupervised learning (clustering), and model evaluation techniques. Questions will test both theoretical understanding and practical application of the concepts.\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(syllabus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311928b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class Lesson(BaseModel):\n",
    "    lesson: str\n",
    "    duration_minutes: int\n",
    "    topics: List[str]\n",
    "\n",
    "\n",
    "class LessonContent(BaseModel):\n",
    "    readings: List[str]\n",
    "    examples: List[str]\n",
    "    exercises: List[str]\n",
    "    assessments: List[str]\n",
    "    additional_resources: List[str]  # NEW FIELD\n",
    "\n",
    "\n",
    "class Unit(BaseModel):\n",
    "    unit_number: int\n",
    "    title: str\n",
    "    unit_description: str\n",
    "    learning_objectives: List[str]\n",
    "    lesson_outline: List[Lesson]\n",
    "    assessment: str\n",
    "    content: Optional[LessonContent] = None\n",
    "\n",
    "\n",
    "class FinalAssessment(BaseModel):\n",
    "    final_exam_description: str\n",
    "    final_project_description: str\n",
    "\n",
    "\n",
    "class Course(BaseModel):\n",
    "    title: str\n",
    "    description: str\n",
    "    estimated_duration_hours: int\n",
    "    prerequisites: List[str]\n",
    "    lesson: List[Unit]\n",
    "    final_assessment: FinalAssessment\n",
    "\n",
    "\n",
    "class Syllabus(BaseModel):\n",
    "    course: Course\n",
    "\n",
    "\n",
    "class LessonTopic(BaseModel):\n",
    "    title: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "809e0141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "\n",
    "# Function to validate a URL using requests\n",
    "def validate_url(url: str) -> bool:\n",
    "    try:\n",
    "        response = requests.head(url, allow_redirects=True, timeout=5)\n",
    "        if response.status_code >= 400 or response.status_code == 405:\n",
    "            response = requests.get(url, allow_redirects=True, timeout=8)\n",
    "        return response.status_code < 400\n",
    "    except requests.RequestException:\n",
    "        return False\n",
    "\n",
    "# Function to extract and validate URLs from the 'Additional Resources' section of lesson content\n",
    "def extract_and_validate_additional_resources(text: str, unit_title: str) -> str:\n",
    "    updated_text = \"\"\n",
    "    inside_resources = False\n",
    "\n",
    "    for line in text.splitlines():\n",
    "        if line.strip().lower().startswith(\"additional resources\"):\n",
    "            inside_resources = True\n",
    "            updated_text += f\"Unit: {unit_title}\\n\"  # Add the unit title before the resources section\n",
    "            updated_text += line + \"\\n\"  # Keep the \"Additional Resources\" header\n",
    "            continue\n",
    "\n",
    "        if inside_resources:\n",
    "            if not line.strip():  # End of the section\n",
    "                updated_text += line + \"\\n\"\n",
    "                break\n",
    "\n",
    "            match = re.search(r'https?://\\S+', line)\n",
    "            if match:\n",
    "                url = match.group(0)\n",
    "                if validate_url(url):\n",
    "                    updated_text += line + \"\\n\"  # Keep valid URLs\n",
    "            # If the URL is invalid, we don't add it to the `updated_text`\n",
    "        else:\n",
    "            updated_text += line + \"\\n\"  # Keep everything outside of \"Additional Resources\"\n",
    "\n",
    "    return updated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "93b316f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lesson_content(lesson_title: str, learning_objectives: List[str], lesson_outline: List[Lesson],unit_title: str):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=[\n",
    "            f\"Generate detailed content for the lesson titled '{lesson_title}' with the following learning objectives: {', '.join(learning_objectives)}.\",\n",
    "            \"Provide readings to help learn the material, practical examples with explanations, exercises for practice, and assessments for evaluation.\",\n",
    "            \"The readings should be clear and comprehensive.\",\n",
    "            \"The examples should demonstrate the concepts effectively.\",\n",
    "            \"The exercises should be practice-oriented and the assessments should be auto-gradable.\",\n",
    "            \"Include a section at the end titled 'Additional Resources:' with 6 helpful and relevant resource links (one per line, starting with a dash).\",\n",
    "            \"Return the content as plain text, separated into sections for 'Readings', 'Examples', 'Exercises', and 'Assessments'.\"\n",
    "        ],\n",
    "        config={\n",
    "            'response_mime_type': 'text/plain', \n",
    "        }\n",
    "    )\n",
    "    updated_text = extract_and_validate_additional_resources(response.text,unit_title)\n",
    "    return updated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "ec74f134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Threaded lesson content generation\n",
    "def generate_lesson_threaded(lesson, unit_learning_objectives, index, all_content, unit_title):\n",
    "    try:\n",
    "        time.sleep(0.5)\n",
    "        lesson_content_text = generate_lesson_content(\n",
    "            lesson_title=lesson[\"lesson\"],\n",
    "            learning_objectives=unit_learning_objectives,\n",
    "            lesson_outline=[lesson],\n",
    "            unit_title=unit_title  # 👈 Pass unit title here\n",
    "        )\n",
    "        if lesson_content_text:\n",
    "            all_content[index] = f\"\\n\\n=== {unit_title} ===\\n\\n{lesson_content_text}\"\n",
    "    except Exception as e:\n",
    "        all_content[index] = f\"Error generating content for lesson {lesson['lesson']}: {e}\"\n",
    "\n",
    "# Main course generator\n",
    "def generate_full_course(syllabus_json_str: str) -> str:\n",
    "    try:\n",
    "        syllabus_data = json.loads(syllabus_json_str)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Invalid JSON input. Error: {e}\")\n",
    "\n",
    "    # Adjusted for your JSON schema: \"lessons\" holds units\n",
    "    total_lessons = sum(len(unit[\"lesson_outline\"]) for unit in syllabus_data[\"course\"][\"lessons\"])\n",
    "    has_final_exam = \"final_assessment\" in syllabus_data[\"course\"] and \\\n",
    "                     \"final_exam_description\" in syllabus_data[\"course\"][\"final_assessment\"]\n",
    "\n",
    "    all_content = [None] * (total_lessons + int(has_final_exam))\n",
    "    threads = []\n",
    "    index = 0\n",
    "\n",
    "    for unit in syllabus_data[\"course\"][\"lessons\"]:\n",
    "        unit_title = f\"UNIT {unit['unit_number']}: {unit['title']}\"\n",
    "        for lesson in unit[\"lesson_outline\"]:\n",
    "            thread = threading.Thread(\n",
    "                target=generate_lesson_threaded,\n",
    "                args=(lesson, unit[\"learning_objectives\"], index, all_content, unit_title)\n",
    "            )\n",
    "            threads.append(thread)\n",
    "            thread.start()\n",
    "            index += 1\n",
    "\n",
    "    # Final Exam\n",
    "    if has_final_exam:\n",
    "        final_exam_description = syllabus_data[\"course\"][\"final_assessment\"][\"final_exam_description\"]\n",
    "\n",
    "        def generate_final_exam(index, all_content):\n",
    "            try:\n",
    "                time.sleep(0.5)\n",
    "                response = client.models.generate_content(\n",
    "                    model=\"gemini-2.0-flash\",\n",
    "                    contents=[\n",
    "                        \"Generate a final exam for the course based on the following description.\",\n",
    "                        final_exam_description,\n",
    "                        \"Include a variety of question types such as multiple choice, short answer, and coding tasks.\",\n",
    "                        \"Make the exam comprehensive and clearly structured.\",\n",
    "                        \"Return this as plain text.\"\n",
    "                    ],\n",
    "                    config={\"response_mime_type\": \"text/plain\"}\n",
    "                )\n",
    "                all_content[index] = \"\\n\\n=== FINAL EXAM ===\\n\\n\" + response.text\n",
    "            except Exception as e:\n",
    "                all_content[index] = f\"Error generating final exam: {e}\"\n",
    "\n",
    "        thread = threading.Thread(\n",
    "            target=generate_final_exam,\n",
    "            args=(index, all_content)\n",
    "        )\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "    # Wait for all threads to complete\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    return \"\\n\".join(content for content in all_content if content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f249bbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = generate_full_course(syllabus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "631d328e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== UNIT 1: Introduction to Machine Learning ===\n",
      "\n",
      "Here's the lesson content, separated into the requested sections:\n",
      "\n",
      "**Readings**\n",
      "\n",
      "**What is Machine Learning?**\n",
      "\n",
      "Machine learning (ML) is a field of computer science that gives computer systems the ability to learn from data without being explicitly programmed. Instead of writing specific instructions for every possible situation, ML algorithms learn patterns and relationships in data to make predictions or decisions. Think of it like teaching a dog: you don't tell it *exactly* how to sit using complex rules, you reward it when it sits correctly and discourage it when it doesn't. ML algorithms work similarly, adjusting their internal parameters based on the data they're exposed to.\n",
      "\n",
      "**Scope of Machine Learning:**\n",
      "\n",
      "Machine learning has a broad scope and is applicable to numerous domains, including:\n",
      "\n",
      "*   **Image Recognition:** Identifying objects in images (e.g., faces, cars, animals).\n",
      "*   **Natural Language Processing (NLP):** Understanding and generating human language (e.g., chatbots, translation).\n",
      "*   **Recommendation Systems:** Suggesting products or content based on user preferences (e.g., Netflix, Amazon).\n",
      "*   **Fraud Detection:** Identifying fraudulent transactions.\n",
      "*   **Medical Diagnosis:** Assisting doctors in diagnosing diseases.\n",
      "*   **Financial Modeling:** Predicting stock prices or assessing credit risk.\n",
      "\n",
      "**Types of Machine Learning:**\n",
      "\n",
      "There are primarily three types of machine learning:\n",
      "\n",
      "1.  **Supervised Learning:** In supervised learning, the algorithm learns from labeled data. This means that the training data includes both the input features and the correct output (label). The algorithm learns a mapping from inputs to outputs so it can predict the output for new, unseen inputs. Examples include:\n",
      "\n",
      "    *   *Classification:* Predicting a category (e.g., spam or not spam).\n",
      "    *   *Regression:* Predicting a continuous value (e.g., house price).\n",
      "\n",
      "2.  **Unsupervised Learning:** In unsupervised learning, the algorithm learns from unlabeled data. The algorithm tries to discover hidden patterns or structures in the data without any prior knowledge of the correct output. Examples include:\n",
      "\n",
      "    *   *Clustering:* Grouping similar data points together (e.g., customer segmentation).\n",
      "    *   *Dimensionality Reduction:* Reducing the number of features while preserving important information (e.g., principal component analysis).\n",
      "    *   *Anomaly Detection:* Identifying unusual data points (e.g., fraudulent transactions).\n",
      "\n",
      "3.  **Reinforcement Learning:** In reinforcement learning, the algorithm learns by interacting with an environment. The algorithm receives rewards or penalties for its actions and learns to choose actions that maximize its cumulative reward over time. Examples include:\n",
      "\n",
      "    *   *Game Playing:* Training an AI to play games like chess or Go.\n",
      "    *   *Robotics:* Controlling a robot to perform tasks.\n",
      "    *   *Resource Management:* Optimizing resource allocation.\n",
      "\n",
      "**Steps in a Typical Machine Learning Project:**\n",
      "\n",
      "A typical machine learning project involves the following steps:\n",
      "\n",
      "1.  **Data Collection:** Gathering relevant data from various sources.\n",
      "2.  **Data Preprocessing:** Cleaning, transforming, and preparing the data for analysis (handling missing values, removing outliers, feature scaling).\n",
      "3.  **Feature Engineering:** Selecting, transforming, or creating new features that improve model performance.\n",
      "4.  **Model Selection:** Choosing an appropriate machine learning algorithm based on the problem and data.\n",
      "5.  **Model Training:** Training the selected model on the training data.\n",
      "6.  **Model Evaluation:** Evaluating the model's performance on a separate test dataset.\n",
      "7.  **Hyperparameter Tuning:** Optimizing the model's parameters to improve performance.\n",
      "8.  **Deployment:** Deploying the trained model to make predictions on new data.\n",
      "9.  **Monitoring and Maintenance:** Continuously monitoring the model's performance and retraining it as needed.\n",
      "\n",
      "**Basic Machine Learning Terminology:**\n",
      "\n",
      "*   **Features:** Input variables used to make predictions (also called independent variables or attributes). For example, in predicting house prices, features might include square footage, number of bedrooms, and location.\n",
      "*   **Labels:** The target variable that the model is trying to predict (also called dependent variable or outcome variable). For example, in predicting house prices, the label is the actual house price.\n",
      "*   **Model:** A mathematical representation of the relationship between features and labels. It is the output of the training process.\n",
      "*   **Training Data:** The data used to train the model.\n",
      "*   **Test Data:** The data used to evaluate the model's performance after training.\n",
      "*   **Algorithm:** The specific mathematical method used to learn the model.\n",
      "*   **Hyperparameters:** Parameters that control the learning process of the model (e.g., learning rate, number of layers in a neural network). These are set *before* training.\n",
      "*   **Prediction:** The output of the model for a given input.\n",
      "*   **Accuracy:** A measure of how well the model is performing (e.g., percentage of correct predictions).\n",
      "*   **Overfitting:** When a model learns the training data too well and performs poorly on new data.  It memorizes the training data instead of generalizing.\n",
      "*   **Underfitting:** When a model is too simple and cannot capture the underlying patterns in the data.\n",
      "\n",
      "**Examples**\n",
      "\n",
      "1.  **Supervised Learning - Classification (Spam Detection):**\n",
      "\n",
      "    *   **Problem:** Build a model to classify emails as spam or not spam.\n",
      "    *   **Features:** Words in the email, sender address, subject line, presence of attachments.\n",
      "    *   **Label:** Spam or Not Spam (0 or 1).\n",
      "    *   **Algorithm:** Logistic Regression, Support Vector Machine (SVM), Naive Bayes.\n",
      "    *   **Explanation:** The model learns the relationship between the features and the label (spam/not spam) from a dataset of labeled emails. New emails are then classified based on this learned relationship.  For instance, if an email contains many words associated with spam (e.g., \"discount,\" \"urgent,\" \"free\"), the model will likely predict it as spam.\n",
      "\n",
      "2.  **Supervised Learning - Regression (House Price Prediction):**\n",
      "\n",
      "    *   **Problem:** Predict the price of a house based on its features.\n",
      "    *   **Features:** Square footage, number of bedrooms, number of bathrooms, location, age of the house.\n",
      "    *   **Label:** House price (in dollars).\n",
      "    *   **Algorithm:** Linear Regression, Decision Tree Regression, Random Forest Regression.\n",
      "    *   **Explanation:** The model learns the relationship between the features and the house price from a dataset of labeled houses. A house with a larger square footage and more bedrooms would be predicted to have a higher price.\n",
      "\n",
      "3.  **Unsupervised Learning - Clustering (Customer Segmentation):**\n",
      "\n",
      "    *   **Problem:** Group customers into different segments based on their purchasing behavior.\n",
      "    *   **Features:** Purchase history, demographics, website activity.\n",
      "    *   **Label:** (None - unsupervised)\n",
      "    *   **Algorithm:** K-Means Clustering, Hierarchical Clustering.\n",
      "    *   **Explanation:** The algorithm identifies clusters of customers with similar purchasing patterns. This information can be used to tailor marketing campaigns to specific customer segments. For example, one cluster might consist of high-spending customers who frequently purchase luxury items, while another cluster might consist of price-sensitive customers who primarily purchase discounted items.\n",
      "\n",
      "4.  **Unsupervised Learning - Anomaly Detection (Fraud Detection):**\n",
      "\n",
      "    *   **Problem:** Identify fraudulent transactions.\n",
      "    *   **Features:** Transaction amount, time of day, location, merchant category.\n",
      "    *   **Label:** (None - unsupervised)\n",
      "    *   **Algorithm:** Isolation Forest, One-Class SVM.\n",
      "    *   **Explanation:** The algorithm identifies transactions that are significantly different from the norm. A transaction with a very large amount, occurring at an unusual time, and from a foreign country might be flagged as potentially fraudulent.\n",
      "\n",
      "5.  **Reinforcement Learning - Game Playing (AI for Chess):**\n",
      "\n",
      "    *   **Problem:** Train an AI to play chess.\n",
      "    *   **Environment:** The chessboard and the rules of chess.\n",
      "    *   **Agent:** The AI player.\n",
      "    *   **Actions:** Possible chess moves.\n",
      "    *   **Reward:** +1 for winning, -1 for losing, 0 for a draw or any intermediate move.\n",
      "    *   **Algorithm:** Q-learning, Deep Q-Network (DQN).\n",
      "    *   **Explanation:** The AI learns to play chess by playing against itself or other players. It receives rewards for making good moves and penalties for making bad moves. Over time, it learns to choose actions that maximize its chances of winning.\n",
      "\n",
      "**Exercises**\n",
      "\n",
      "1.  **Identify the Type of Machine Learning:** For each of the following scenarios, identify whether it's a supervised, unsupervised, or reinforcement learning problem. Explain your reasoning.\n",
      "\n",
      "    *   a) Predicting whether a customer will churn (cancel their subscription) based on their usage patterns.\n",
      "    *   b) Grouping news articles into different topics based on their content.\n",
      "    *   c) Training a robot to navigate a maze.\n",
      "    *   d) Predicting the temperature for tomorrow based on historical weather data.\n",
      "    *   e) Identifying defective products on a production line. The system only sees images of products, and flags the \"different\" ones.\n",
      "\n",
      "2.  **Feature Identification:** For the following problem, identify at least three relevant features that could be used to build a machine learning model:\n",
      "\n",
      "    *   Problem: Predicting whether a student will pass an exam.\n",
      "\n",
      "3.  **Terminology Matching:** Match the following terms with their definitions:\n",
      "\n",
      "    *   Terms: Feature, Label, Model, Training Data, Testing Data\n",
      "    *   Definitions:\n",
      "        *   a) The data used to evaluate the model's performance.\n",
      "        *   b) The target variable the model is trying to predict.\n",
      "        *   c) The input variable used to make predictions.\n",
      "        *   d) The data used to train the model.\n",
      "        *   e) A mathematical representation of the relationship between features and labels.\n",
      "\n",
      "4.  **Scenario Application:** Suppose you want to build a system to recommend movies to users.  Describe which type of machine learning (supervised, unsupervised, or reinforcement) you would use and why. Briefly outline the steps you would take in building this system.\n",
      "\n",
      "**Assessments**\n",
      "\n",
      "The following are multiple-choice questions. Each correct answer is worth 1 point.\n",
      "\n",
      "1.  Which of the following is the *best* definition of machine learning?\n",
      "    a) Writing explicit instructions for a computer to perform a task.\n",
      "    b) Enabling computers to learn from data without being explicitly programmed.\n",
      "    c) Creating static programs that always produce the same output.\n",
      "    d) Using pre-defined rules to make decisions.\n",
      "\n",
      "2.  Which type of machine learning involves training a model on labeled data?\n",
      "    a) Unsupervised learning\n",
      "    b) Reinforcement learning\n",
      "    c) Supervised learning\n",
      "    d) None of the above\n",
      "\n",
      "3.  Clustering is an example of:\n",
      "    a) Supervised learning\n",
      "    b) Reinforcement learning\n",
      "    c) Unsupervised learning\n",
      "    d) Regression\n",
      "\n",
      "4.  In machine learning, what are the input variables used to make predictions called?\n",
      "    a) Labels\n",
      "    b) Models\n",
      "    c) Features\n",
      "    d) Algorithms\n",
      "\n",
      "5.  What is the purpose of the 'test data' in a machine learning project?\n",
      "    a) To train the model.\n",
      "    b) To evaluate the model's performance after training.\n",
      "    c) To collect the data.\n",
      "    d) To preprocess the data.\n",
      "\n",
      "6.  Which of the following scenarios is *best* suited for reinforcement learning?\n",
      "    a) Predicting the price of a used car.\n",
      "    b) Training an AI to play a video game.\n",
      "    c) Grouping customers into different segments.\n",
      "    d) Detecting fraudulent credit card transactions.\n",
      "\n",
      "7. Which of the following represents a common problem in Machine Learning where the model learns the training data too well, resulting in poor performance on new, unseen data?\n",
      "    a) Data Bias\n",
      "    b) Underfitting\n",
      "    c) Overfitting\n",
      "    d) Feature Scaling\n",
      "\n",
      "**Answer Key:**\n",
      "\n",
      "1.  b\n",
      "2.  c\n",
      "3.  c\n",
      "4.  c\n",
      "5.  b\n",
      "6.  b\n",
      "7.  c\n",
      "\n",
      "**Additional Resources:**\n",
      "\n",
      "- https://www.coursera.org/learn/machine-learning\n",
      "- https://developers.google.com/machine-learning/crash-course\n",
      "- https://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
      "- https://www.tensorflow.org/tutorials\n",
      "- https://pytorch.org/tutorials/\n",
      "- https://www.deeplearning.ai/\n",
      "\n",
      "\n",
      "\n",
      "=== UNIT 1: Introduction to Machine Learning ===\n",
      "\n",
      "```\n",
      "## The Machine Learning Process\n",
      "\n",
      "**Learning Objectives:**\n",
      "\n",
      "*   Understand the definition and scope of machine learning.\n",
      "*   Distinguish between supervised, unsupervised, and reinforcement learning.\n",
      "*   Describe the steps involved in a typical machine learning project.\n",
      "*   Learn basic machine learning terminology (features, labels, models, etc.).\n",
      "\n",
      "---\n",
      "\n",
      "### Readings\n",
      "\n",
      "**1. Introduction to Machine Learning**\n",
      "\n",
      "Machine learning (ML) is a field of computer science that gives computer systems the ability to learn from data without being explicitly programmed. Instead of writing specific rules for a computer to follow, ML algorithms learn patterns and relationships from data and use those patterns to make predictions or decisions about new, unseen data. This learning process allows the system to improve its performance over time as it is exposed to more data.\n",
      "\n",
      "The scope of machine learning is vast, encompassing various applications such as image recognition, natural language processing, fraud detection, recommendation systems, and many more. The core idea is to automate tasks that typically require human intelligence by leveraging statistical methods and computational algorithms.\n",
      "\n",
      "**2. Types of Machine Learning**\n",
      "\n",
      "There are three main types of machine learning:\n",
      "\n",
      "*   **Supervised Learning:** In supervised learning, the algorithm learns from labeled data, meaning the data includes both the input features and the desired output (label). The goal is to learn a mapping function that can predict the output for new, unseen inputs. Examples include predicting housing prices based on features like size and location (regression) or classifying emails as spam or not spam (classification).\n",
      "    *   **Regression:** Predicting a continuous value (e.g., stock prices, temperature).\n",
      "    *   **Classification:** Predicting a categorical value (e.g., spam/not spam, cat/dog).\n",
      "\n",
      "*   **Unsupervised Learning:** In unsupervised learning, the algorithm learns from unlabeled data, meaning the data only includes input features without any predefined output. The goal is to discover hidden patterns, structures, or relationships within the data. Examples include clustering customers into different segments based on their purchasing behavior or reducing the dimensionality of a dataset to simplify analysis.\n",
      "    *   **Clustering:** Grouping similar data points together (e.g., customer segmentation).\n",
      "    *   **Dimensionality Reduction:** Reducing the number of variables while preserving important information (e.g., Principal Component Analysis).\n",
      "\n",
      "*   **Reinforcement Learning:** In reinforcement learning, the algorithm learns by interacting with an environment and receiving rewards or penalties for its actions. The goal is to learn an optimal policy that maximizes the cumulative reward over time. Examples include training a robot to navigate a maze or developing a game-playing AI.\n",
      "    *   **Agent:** The learner that interacts with the environment.\n",
      "    *   **Environment:** The world in which the agent operates.\n",
      "    *   **Actions:** The choices the agent can make.\n",
      "    *   **Reward:** Feedback from the environment indicating the desirability of an action.\n",
      "    *   **Policy:** The strategy the agent uses to choose actions.\n",
      "\n",
      "**3. The Machine Learning Project Lifecycle**\n",
      "\n",
      "A typical machine learning project involves the following steps:\n",
      "\n",
      "1.  **Problem Definition:** Clearly define the problem you are trying to solve and identify the desired outcome.  What questions are you trying to answer? What data is available?\n",
      "\n",
      "2.  **Data Collection:** Gather relevant data from various sources.  Ensure the data is representative of the problem you're trying to solve.\n",
      "\n",
      "3.  **Data Preprocessing:** Clean, transform, and prepare the data for analysis. This may involve handling missing values, removing outliers, scaling features, and encoding categorical variables.  This is often the most time-consuming step.\n",
      "\n",
      "4.  **Feature Engineering:** Create new features or transform existing ones to improve the performance of the model. This requires domain knowledge and creativity.\n",
      "\n",
      "5.  **Model Selection:** Choose an appropriate machine learning algorithm based on the problem type (supervised, unsupervised, reinforcement) and the characteristics of the data.\n",
      "\n",
      "6.  **Model Training:** Train the selected model using the preprocessed data.  This involves feeding the data to the algorithm and allowing it to learn the patterns and relationships.\n",
      "\n",
      "7.  **Model Evaluation:** Evaluate the performance of the trained model using a separate dataset (validation or test set).  Use appropriate metrics to assess the model's accuracy, precision, recall, etc.\n",
      "\n",
      "8.  **Model Tuning (Hyperparameter Optimization):** Optimize the model's hyperparameters to improve its performance.  This involves trying different combinations of hyperparameters and selecting the ones that yield the best results.\n",
      "\n",
      "9.  **Model Deployment:** Deploy the trained model to a production environment where it can be used to make predictions on new data.\n",
      "\n",
      "10. **Monitoring and Maintenance:** Continuously monitor the model's performance in the production environment and retrain the model periodically with new data to maintain its accuracy.\n",
      "\n",
      "**4. Basic Machine Learning Terminology**\n",
      "\n",
      "*   **Features:** The input variables used to train the model (also known as independent variables or attributes). For example, in predicting housing prices, features might include square footage, number of bedrooms, and location.\n",
      "\n",
      "*   **Labels:** The output variable that the model is trying to predict (also known as dependent variable or target variable). For example, in predicting housing prices, the label would be the actual price of the house.\n",
      "\n",
      "*   **Model:** A mathematical representation of the relationships between the features and the labels, learned from the training data.\n",
      "\n",
      "*   **Algorithm:** The specific method used to train the model (e.g., linear regression, decision tree, neural network).\n",
      "\n",
      "*   **Training Data:** The data used to train the model.\n",
      "\n",
      "*   **Testing Data:** The data used to evaluate the performance of the trained model on unseen data.\n",
      "\n",
      "*   **Validation Data:** Data used to tune model hyperparameters.  Sometimes validation data is combined with training data using techniques like k-fold cross-validation.\n",
      "\n",
      "*   **Hyperparameters:** Parameters that are set *before* training and control the learning process (e.g., learning rate, number of layers in a neural network).  These are *not* learned from the data.\n",
      "\n",
      "*   **Accuracy:** The proportion of correctly classified instances.\n",
      "\n",
      "*   **Precision:** The proportion of positive predictions that are actually correct.\n",
      "\n",
      "*   **Recall:** The proportion of actual positive instances that are correctly predicted.\n",
      "\n",
      "*   **F1-Score:** The harmonic mean of precision and recall, providing a balanced measure of performance.\n",
      "\n",
      "---\n",
      "\n",
      "### Examples\n",
      "\n",
      "**1. Supervised Learning: Predicting Housing Prices (Regression)**\n",
      "\n",
      "Imagine you have a dataset of houses with features like square footage, number of bedrooms, number of bathrooms, and location. The label is the price of the house. You can use a supervised learning algorithm like linear regression to train a model that predicts the price of a house based on its features.\n",
      "\n",
      "*   **Features:** Square footage, number of bedrooms, number of bathrooms, location (encoded numerically).\n",
      "*   **Label:** Price of the house.\n",
      "*   **Algorithm:** Linear Regression.\n",
      "\n",
      "The model learns a linear relationship between the features and the price. For example, it might learn that each additional square foot increases the price by $100, and each additional bedroom increases the price by $50,000.\n",
      "\n",
      "**2. Supervised Learning: Email Spam Detection (Classification)**\n",
      "\n",
      "You have a dataset of emails, each labeled as either \"spam\" or \"not spam.\" You can use a supervised learning algorithm like a Naive Bayes classifier to train a model that predicts whether a new email is spam based on its content.\n",
      "\n",
      "*   **Features:** Words in the email (e.g., \"free,\" \"discount,\" \"urgent\").\n",
      "*   **Label:** Spam or Not Spam.\n",
      "*   **Algorithm:** Naive Bayes.\n",
      "\n",
      "The model learns the probability of each word appearing in spam emails versus legitimate emails. If an email contains many words that are commonly found in spam, the model will predict that it is spam.\n",
      "\n",
      "**3. Unsupervised Learning: Customer Segmentation (Clustering)**\n",
      "\n",
      "You have a dataset of customers with features like age, income, purchase history, and website activity. You can use an unsupervised learning algorithm like k-means clustering to group customers into different segments based on their similarities.\n",
      "\n",
      "*   **Features:** Age, income, purchase history, website activity.\n",
      "*   **Label:** *None* (Unsupervised Learning).\n",
      "*   **Algorithm:** K-Means Clustering.\n",
      "\n",
      "The algorithm finds clusters of customers who are similar to each other. For example, one cluster might be young, high-income customers who frequently purchase luxury goods, while another cluster might be older, low-income customers who primarily purchase essential items.\n",
      "\n",
      "**4. Reinforcement Learning: Training a Game-Playing AI**\n",
      "\n",
      "You want to train an AI to play a game like chess. The AI acts as an agent interacting with the game environment. After each move (action), the agent receives a reward (e.g., +1 for capturing a piece, -1 for losing a piece, +10 for winning the game). The AI learns a policy that maximizes its cumulative reward over time.\n",
      "\n",
      "*   **Agent:** The game-playing AI.\n",
      "*   **Environment:** The chess board and game rules.\n",
      "*   **Actions:** Possible moves the AI can make.\n",
      "*   **Reward:** Points awarded or deducted based on the outcome of moves.\n",
      "*   **Algorithm:** Q-learning.\n",
      "\n",
      "**5. Data Preprocessing Example: Handling Missing Values**\n",
      "\n",
      "Suppose you have a dataset of patient information with a column for 'Age'. Some patients have missing Age values.\n",
      "\n",
      "*   **Options:**\n",
      "    *   **Deletion:** Remove rows with missing Age values. (Use with caution, can lose valuable data.)\n",
      "    *   **Imputation (Mean/Median):** Replace missing values with the mean or median of the existing Age values.  Median is often preferred if the Age distribution is skewed.\n",
      "    *   **Imputation (Mode):** Replace missing values with the most frequent Age value.\n",
      "    *   **Model-Based Imputation:** Use a machine learning model (e.g., regression) to predict the missing Age values based on other features.\n",
      "\n",
      "**6. Feature Engineering Example: Creating Interaction Features**\n",
      "\n",
      "You are predicting house prices. You have features for 'Square Footage' and 'Number of Bedrooms'. You could create a new feature 'Square Footage per Bedroom' by dividing 'Square Footage' by 'Number of Bedrooms'. This new feature might capture the density of the house and be more predictive of price than the individual features alone.\n",
      "\n",
      "---\n",
      "\n",
      "### Exercises\n",
      "\n",
      "**1. Identifying Machine Learning Types**\n",
      "\n",
      "For each of the following scenarios, identify the type of machine learning (supervised, unsupervised, or reinforcement learning) and briefly explain your reasoning.\n",
      "\n",
      "*   a) Predicting whether a customer will default on a loan based on their credit history.\n",
      "*   b) Grouping news articles into different topics based on their content.\n",
      "*   c) Training an AI to play a video game by rewarding it for achieving high scores.\n",
      "*   d) Recommending products to customers based on their past purchases.\n",
      "*   e) Identifying anomalies in sensor data from a manufacturing plant.\n",
      "\n",
      "**2. Machine Learning Project Steps**\n",
      "\n",
      "Describe the steps you would take to build a machine learning model to predict customer churn (i.e., whether a customer will stop using a service).  Be specific about each step and the tasks involved.\n",
      "\n",
      "**3. Feature Engineering Practice**\n",
      "\n",
      "You are given a dataset of information about cars. The features include 'Engine Size (cubic centimeters)' and 'Fuel Efficiency (miles per gallon)'. Describe at least two new features you could engineer from these existing features that might be useful for predicting the car's price. Explain the reasoning behind creating these features.\n",
      "\n",
      "**4. Data Preprocessing Scenario**\n",
      "\n",
      "You have a dataset with a feature called 'Salary'. Some values in the 'Salary' column are missing. Some values are expressed in USD, some in EUR, and some in GBP.  Describe the steps you would take to preprocess this 'Salary' feature to make it suitable for machine learning.\n",
      "\n",
      "**5. Terminology Matching**\n",
      "\n",
      "Match the following terms with their definitions:\n",
      "\n",
      "*   Term: Feature, Label, Model, Algorithm, Training Data\n",
      "*   Definitions:\n",
      "    *   A mathematical representation of the relationships between features and labels.\n",
      "    *   The data used to train the model.\n",
      "    *   An input variable used to train the model.\n",
      "    *   The output variable that the model is trying to predict.\n",
      "    *   The specific method used to train the model.\n",
      "\n",
      "---\n",
      "\n",
      "### Assessments\n",
      "\n",
      "**Instructions:** Choose the best answer for each question.\n",
      "\n",
      "**1. What is the primary goal of machine learning?**\n",
      "a) To write specific rules for computers to follow.\n",
      "b) To enable computers to learn from data without explicit programming.\n",
      "c) To create static programs that do not change over time.\n",
      "d) To replace human intelligence entirely.\n",
      "\n",
      "**2. Which type of machine learning involves learning from labeled data?**\n",
      "a) Unsupervised learning\n",
      "b) Reinforcement learning\n",
      "c) Supervised learning\n",
      "d) Semi-supervised learning\n",
      "\n",
      "**3. What is the purpose of data preprocessing in a machine learning project?**\n",
      "a) To select the best machine learning algorithm.\n",
      "b) To clean, transform, and prepare the data for analysis.\n",
      "c) To deploy the trained model to a production environment.\n",
      "d) To evaluate the performance of the trained model.\n",
      "\n",
      "**4. In machine learning terminology, what is a \"feature\"?**\n",
      "a) The output variable the model is trying to predict.\n",
      "b) An input variable used to train the model.\n",
      "c) The specific method used to train the model.\n",
      "d) A parameter that is learned during training.\n",
      "\n",
      "**5. Which of the following is NOT a typical step in a machine learning project lifecycle?**\n",
      "a) Data collection\n",
      "b) Model training\n",
      "c) Problem definition\n",
      "d) Operating System Installation\n",
      "\n",
      "**6. What is the primary goal of unsupervised learning?**\n",
      "a) To predict future outcomes with labeled data.\n",
      "b) To discover hidden patterns and structures in unlabeled data.\n",
      "c) To train an agent to maximize rewards in an environment.\n",
      "d) To create specific rules for a computer to follow.\n",
      "\n",
      "**7. In reinforcement learning, what is the \"environment\"?**\n",
      "a) The learner that interacts with the world.\n",
      "b) The set of possible actions the agent can take.\n",
      "c) The world in which the agent operates.\n",
      "d) The feedback from the environment indicating the desirability of an action.\n",
      "\n",
      "**8. What is the purpose of a validation dataset?**\n",
      "a) To train the model\n",
      "b) To evaluate the final model performance after training and hyperparameter tuning\n",
      "c) To tune model hyperparameters\n",
      "d) To collect the original data\n",
      "\n",
      "**9. Which of the following best describes \"overfitting\"?**\n",
      "a) The model performs poorly on both training and testing data.\n",
      "b) The model performs well on training data but poorly on testing data.\n",
      "c) The model performs well on testing data but poorly on training data.\n",
      "d) The model is perfectly accurate on all data.\n",
      "\n",
      "**10. What is \"precision\" in the context of classification?**\n",
      "a) The proportion of correctly classified instances.\n",
      "b) The proportion of positive predictions that are actually correct.\n",
      "c) The proportion of actual positive instances that are correctly predicted.\n",
      "d) The harmonic mean of precision and recall.\n",
      "\n",
      "**Answer Key:**\n",
      "1. b\n",
      "2. c\n",
      "3. b\n",
      "4. b\n",
      "5. d\n",
      "6. b\n",
      "7. c\n",
      "8. c\n",
      "9. b\n",
      "10. b\n",
      "\n",
      "---\n",
      "\n",
      "### Additional Resources:\n",
      "\n",
      "- https://www.coursera.org/learn/machine-learning\n",
      "- https://developers.google.com/machine-learning/crash-course\n",
      "- https://scikit-learn.org/stable/tutorial/index.html\n",
      "- https://www.tensorflow.org/tutorials\n",
      "- https://pytorch.org/tutorials/\n",
      "- https://www.kaggle.com/learn/overview\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "=== UNIT 1: Introduction to Machine Learning ===\n",
      "\n",
      "Here's a detailed lesson on the Bias-Variance Tradeoff, designed for comprehensive learning and easy assessment.\n",
      "\n",
      "**Lesson Title: The Bias-Variance Tradeoff**\n",
      "\n",
      "**Learning Objectives:**\n",
      "\n",
      "*   Understand the definition and scope of machine learning.\n",
      "*   Distinguish between supervised, unsupervised, and reinforcement learning.\n",
      "*   Describe the steps involved in a typical machine learning project.\n",
      "*   Learn basic machine learning terminology (features, labels, models, etc.).\n",
      "*   Understand the concept of the Bias-Variance Tradeoff.\n",
      "*   Identify and explain the causes of high bias and high variance.\n",
      "*   Describe techniques to mitigate bias and variance.\n",
      "*   Understand the role of model complexity in the tradeoff.\n",
      "\n",
      "**Readings:**\n",
      "\n",
      "**1. Introduction to Machine Learning:**\n",
      "\n",
      "Machine learning (ML) is a field of computer science that gives computer systems the ability to learn from data without being explicitly programmed. It allows systems to automatically improve from experience.  Think of it as programming by example, rather than providing explicit step-by-step instructions.\n",
      "\n",
      "**Scope of Machine Learning:**\n",
      "\n",
      "Machine learning is used in a vast range of applications, including:\n",
      "\n",
      "*   **Image Recognition:** Identifying objects and people in images (e.g., facial recognition, medical image analysis).\n",
      "*   **Natural Language Processing (NLP):** Understanding and generating human language (e.g., chatbots, sentiment analysis, machine translation).\n",
      "*   **Recommendation Systems:** Suggesting products, movies, or music based on user preferences (e.g., Netflix, Amazon).\n",
      "*   **Fraud Detection:** Identifying fraudulent transactions in financial data.\n",
      "*   **Autonomous Vehicles:** Enabling self-driving cars to perceive and navigate their environment.\n",
      "*   **Medical Diagnosis:** Assisting doctors in diagnosing diseases based on patient data.\n",
      "\n",
      "**2. Types of Machine Learning:**\n",
      "\n",
      "There are three main types of machine learning:\n",
      "\n",
      "*   **Supervised Learning:** The algorithm learns from a labeled dataset, which means the input data is paired with corresponding output labels. The goal is to learn a mapping function that can predict the output label for new, unseen input data.\n",
      "    *   **Examples:** Predicting house prices based on features like size and location (regression), classifying emails as spam or not spam (classification).\n",
      "    *   **Key Concept:** Labeled data, prediction or classification.\n",
      "\n",
      "*   **Unsupervised Learning:** The algorithm learns from an unlabeled dataset, without any predefined output labels. The goal is to discover hidden patterns, structures, or relationships in the data.\n",
      "    *   **Examples:** Clustering customers into different segments based on their purchasing behavior, dimensionality reduction to simplify data while preserving important information.\n",
      "    *   **Key Concept:** Unlabeled data, pattern discovery, structure identification.\n",
      "\n",
      "*   **Reinforcement Learning:** The algorithm learns to make decisions in an environment to maximize a reward. The algorithm learns through trial and error, receiving feedback in the form of rewards or penalties for its actions.\n",
      "    *   **Examples:** Training a computer to play games like chess or Go, controlling a robot to navigate a maze.\n",
      "    *   **Key Concept:** Agent, environment, reward, policy.\n",
      "\n",
      "**3. The Machine Learning Project Lifecycle:**\n",
      "\n",
      "A typical machine learning project involves the following steps:\n",
      "\n",
      "1.  **Data Collection:** Gathering relevant data from various sources.\n",
      "2.  **Data Preprocessing:** Cleaning, transforming, and preparing the data for analysis. This includes handling missing values, removing outliers, and scaling features.\n",
      "3.  **Feature Engineering:** Selecting or creating relevant features that can improve model performance.\n",
      "4.  **Model Selection:** Choosing an appropriate machine learning algorithm for the task.\n",
      "5.  **Model Training:** Training the selected model on the training data.\n",
      "6.  **Model Evaluation:** Evaluating the performance of the trained model on a separate test dataset.\n",
      "7.  **Hyperparameter Tuning:** Optimizing the model's hyperparameters to improve its performance.\n",
      "8.  **Model Deployment:** Deploying the trained model to a production environment.\n",
      "9.  **Monitoring and Maintenance:** Continuously monitoring the model's performance and retraining it as needed.\n",
      "\n",
      "**4. Basic Machine Learning Terminology:**\n",
      "\n",
      "*   **Features:** The input variables used to train the model (also known as independent variables or predictors).\n",
      "*   **Labels:** The output variables that the model is trying to predict (also known as dependent variables or target variables).\n",
      "*   **Model:** The algorithm that learns the relationship between the features and the labels.\n",
      "*   **Training Data:** The data used to train the model.\n",
      "*   **Test Data:** The data used to evaluate the performance of the trained model.\n",
      "*   **Hyperparameters:** Parameters that control the learning process of the model (e.g., learning rate, number of hidden layers). These are set *before* training.\n",
      "*   **Parameters:** Values learned by the model during training.\n",
      "\n",
      "**5. The Bias-Variance Tradeoff:**\n",
      "\n",
      "The bias-variance tradeoff is a fundamental concept in machine learning. It describes the relationship between a model's ability to fit the training data (low bias) and its ability to generalize to unseen data (low variance).\n",
      "\n",
      "*   **Bias:** Represents the error introduced by approximating a real-world problem, which is often complex, by a simplified model. A model with high bias makes strong assumptions about the data and may underfit the training data. Underfitting means the model performs poorly on both the training and test sets. Linear models are often high bias.\n",
      "\n",
      "*   **Variance:** Represents the model's sensitivity to small variations in the training data. A model with high variance is overly complex and may overfit the training data. Overfitting means the model performs well on the training set but poorly on the test set. Decision Trees are often high variance.\n",
      "\n",
      "*   **The Tradeoff:** The goal is to find a model that balances bias and variance to achieve good performance on unseen data. A model with low bias and low variance is ideal, but in practice, there is often a tradeoff between the two. Decreasing bias often increases variance, and vice versa.\n",
      "\n",
      "**6. Causes of High Bias and High Variance:**\n",
      "\n",
      "*   **High Bias:**\n",
      "    *   **Oversimplified Model:** Using a model that is too simple to capture the underlying patterns in the data.  For example, using a linear model to fit highly non-linear data.\n",
      "    *   **Insufficient Features:** Not including enough relevant features in the training data.\n",
      "    *   **Strong Assumptions:** Making strong assumptions about the data that are not valid.\n",
      "    *   **Underfitting:** The model fails to capture the underlying relationship between features and labels.\n",
      "\n",
      "*   **High Variance:**\n",
      "    *   **Overly Complex Model:** Using a model that is too complex and captures noise in the training data.\n",
      "    *   **Too Many Features:** Including irrelevant or redundant features in the training data.\n",
      "    *   **Limited Training Data:** Training the model on a small dataset, which may not be representative of the population.\n",
      "    *   **Overfitting:** The model learns the training data too well, including noise and outliers, leading to poor generalization on new data.\n",
      "\n",
      "**7. Techniques to Mitigate Bias and Variance:**\n",
      "\n",
      "*   **Mitigating High Bias:**\n",
      "    *   **Use a more complex model:** Try a model with more parameters or flexibility (e.g., switch from linear regression to polynomial regression, or from a shallow neural network to a deeper one).\n",
      "    *   **Add more features:** Include more relevant features in the training data.\n",
      "    *   **Reduce regularization:** Regularization techniques (e.g., L1, L2) can increase bias. Reduce the strength of regularization.\n",
      "\n",
      "*   **Mitigating High Variance:**\n",
      "    *   **Use a simpler model:** Try a model with fewer parameters or less flexibility (e.g., switch from a deep neural network to a shallow one).\n",
      "    *   **Reduce the number of features:** Select a subset of the most relevant features (feature selection) or use dimensionality reduction techniques (e.g., PCA).\n",
      "    *   **Increase the amount of training data:** More data can help the model generalize better.\n",
      "    *   **Use regularization:** Regularization techniques can help prevent overfitting.\n",
      "    *   **Cross-validation:** Use techniques like k-fold cross-validation to get a more robust estimate of the model's performance and detect overfitting.\n",
      "    *   **Ensemble methods:** Combine multiple models to reduce variance (e.g., bagging, random forests).\n",
      "\n",
      "**8. Model Complexity and the Tradeoff:**\n",
      "\n",
      "Model complexity plays a crucial role in the bias-variance tradeoff.\n",
      "\n",
      "*   **Simple Models:** Tend to have high bias and low variance. They may underfit the data but are less likely to overfit.\n",
      "*   **Complex Models:** Tend to have low bias and high variance. They can fit the training data well but are more likely to overfit and generalize poorly to new data.\n",
      "\n",
      "The optimal model complexity is the point where the model achieves the best balance between bias and variance, resulting in the lowest overall error on unseen data.\n",
      "\n",
      "**Examples:**\n",
      "\n",
      "**1. Polynomial Regression:**\n",
      "\n",
      "Imagine trying to fit a curve to a dataset.\n",
      "\n",
      "*   **Linear Regression (High Bias):** A straight line will likely not fit the curve well. It's an oversimplified model and has high bias.  It won't capture the non-linear relationship.\n",
      "*   **High-Degree Polynomial (High Variance):** A very wiggly curve that passes through every data point will perfectly fit the training data. However, it will likely perform poorly on new data because it's fitting the noise. It has high variance.\n",
      "*   **Moderate-Degree Polynomial (Good Balance):** A curve that captures the general shape of the data without fitting the noise will likely perform best. It has a good balance of bias and variance.\n",
      "\n",
      "**2. Decision Tree:**\n",
      "\n",
      "*   **Shallow Decision Tree (High Bias):** A tree with only a few levels will make simple decisions and may not capture the complexity of the data.\n",
      "*   **Deep Decision Tree (High Variance):** A tree with many levels can make very specific decisions and may overfit the training data. It captures noise.\n",
      "*   **Pruned Decision Tree (Good Balance):** A tree that is limited in depth or pruned to remove unnecessary branches will perform better on new data. Pruning reduces variance.\n",
      "\n",
      "**3. K-Nearest Neighbors (KNN):**\n",
      "\n",
      "*   **Small K (High Variance):**  With a small number of neighbors, KNN is sensitive to local noise and can lead to overfitting.\n",
      "*   **Large K (High Bias):**  With a large number of neighbors, KNN averages over a wider area and can smooth out important details in the data, leading to underfitting.\n",
      "\n",
      "**Exercises:**\n",
      "\n",
      "1.  **Bias/Variance Identification:** For each of the following scenarios, identify whether the model likely suffers from high bias or high variance:\n",
      "\n",
      "    *   A model performs poorly on both the training and test datasets.\n",
      "    *   A model performs very well on the training dataset but poorly on the test dataset.\n",
      "    *   A model's performance on the test dataset improves significantly after adding more features.\n",
      "    *   A model's performance on the test dataset improves significantly after using regularization.\n",
      "\n",
      "2.  **Model Selection:** You are building a model to predict customer churn. You have two models:\n",
      "\n",
      "    *   Model A: A simple logistic regression model.\n",
      "    *   Model B: A complex deep neural network.\n",
      "\n",
      "    You find that Model A performs poorly on both the training and test datasets, while Model B performs very well on the training dataset but poorly on the test dataset. Which model is likely suffering from high bias, and which is likely suffering from high variance? What steps could you take to improve each model?\n",
      "\n",
      "3.  **Regularization:** Explain how L1 and L2 regularization can help to mitigate high variance. How do they affect the complexity of the model?\n",
      "\n",
      "4. **Data Size:** Explain why increasing the size of the training dataset can sometimes help to reduce variance.\n",
      "\n",
      "**Assessments:**\n",
      "\n",
      "**Instructions:** Choose the best answer for each question.\n",
      "\n",
      "1.  **Which of the following is a characteristic of a model with high bias?**\n",
      "\n",
      "    a)  It overfits the training data.\n",
      "    b)  It underfits the training data.\n",
      "    c)  It generalizes well to unseen data.\n",
      "    d)  It is very sensitive to noise in the training data.\n",
      "    *Answer: b)*\n",
      "\n",
      "2.  **Which of the following is a characteristic of a model with high variance?**\n",
      "\n",
      "    a)  It underfits the training data.\n",
      "    b)  It is not sensitive to changes in the training data.\n",
      "    c)  It performs well on the training data but poorly on the test data.\n",
      "    d)  It makes strong assumptions about the data.\n",
      "    *Answer: c)*\n",
      "\n",
      "3.  **Which of the following techniques can be used to mitigate high bias?**\n",
      "\n",
      "    a)  Adding more training data.\n",
      "    b)  Using a simpler model.\n",
      "    c)  Increasing the regularization strength.\n",
      "    d)  Using a more complex model.\n",
      "    *Answer: d)*\n",
      "\n",
      "4.  **Which of the following techniques can be used to mitigate high variance?**\n",
      "\n",
      "    a)  Removing regularization.\n",
      "    b)  Using a more complex model.\n",
      "    c)  Increasing the number of features.\n",
      "    d)  Increasing the regularization strength.\n",
      "    *Answer: d)*\n",
      "\n",
      "5.  **What does the bias-variance tradeoff describe?**\n",
      "\n",
      "    a)  The relationship between the number of features and the size of the training data.\n",
      "    b)  The relationship between a model's ability to fit the training data and its ability to generalize to unseen data.\n",
      "    c)  The relationship between the training time and the inference time of a model.\n",
      "    d)  The relationship between the complexity of a model and its interpretability.\n",
      "    *Answer: b)*\n",
      "\n",
      "6.  **Increasing model complexity typically leads to:**\n",
      "\n",
      "    a) Higher bias and lower variance\n",
      "    b) Lower bias and higher variance\n",
      "    c) Both higher bias and higher variance\n",
      "    d) Both lower bias and lower variance\n",
      "    *Answer: b)*\n",
      "\n",
      "**Additional Resources:**\n",
      "\n",
      "- [https://elitedatascience.com/bias-variance-tradeoff](https://elitedatascience.com/bias-variance-tradeoff)\n",
      "- [https://www.geeksforgeeks.org/understanding-the-bias-variance-tradeoff/](https://www.geeksforgeeks.org/understanding-the-bias-variance-tradeoff/)\n",
      "- [https://www.analyticsvidhya.com/blog/2020/08/understanding-the-concept-of-bias-variance-tradeoff/](https://www.analyticsvidhya.com/blog/2020/08/understanding-the-concept-of-bias-variance-tradeoff/)\n",
      "- [https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off/](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off/)\n",
      "- [https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-in-machine-learning-815a4697432](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-in-machine-learning-815a4697432)\n",
      "- [https://www.ibm.com/cloud/learn/bias-variance](https://www.ibm.com/cloud/learn/bias-variance)\n",
      "\n",
      "\n",
      "\n",
      "=== UNIT 2: Supervised Learning: Regression ===\n",
      "\n",
      "Here's the content for a lesson on Simple Linear Regression, designed to be comprehensive and practical:\n",
      "\n",
      "**Lesson Title: Simple Linear Regression**\n",
      "\n",
      "**Learning Objectives:**\n",
      "\n",
      "*   Understand the principles of linear regression.\n",
      "*   Implement linear regression using Python libraries (e.g., Scikit-learn).\n",
      "*   Evaluate regression models using appropriate metrics (MSE, RMSE, R-squared).\n",
      "*   Learn about regularization techniques to prevent overfitting (L1, L2 regularization).\n",
      "\n",
      "**Readings:**\n",
      "\n",
      "**Introduction to Simple Linear Regression**\n",
      "\n",
      "Simple linear regression is a statistical method that allows us to summarize and study relationships between two continuous (quantitative) variables: one variable, denoted *x*, is regarded as the *predictor, independent variable, or feature*, and the other variable, denoted *y*, is regarded as the *response, dependent variable, or target variable*.\n",
      "\n",
      "The goal of simple linear regression is to find the \"best fit\" line that describes how *y* changes as *x* changes.  This line is represented by the equation:\n",
      "\n",
      "*y = β₀ + β₁x*\n",
      "\n",
      "Where:\n",
      "\n",
      "*   *y* is the predicted value of the dependent variable.\n",
      "*   *x* is the independent variable.\n",
      "*   *β₀* (beta zero) is the y-intercept (the value of *y* when *x* is 0).\n",
      "*   *β₁* (beta one) is the slope of the line (the change in *y* for every one-unit change in *x*).\n",
      "\n",
      "**Estimating the Regression Coefficients (β₀ and β₁)**\n",
      "\n",
      "The primary goal of linear regression is to estimate the values of *β₀* and *β₁* that minimize the difference between the observed values of *y* and the values predicted by the regression line. The most common method used for this is the *least squares* method.\n",
      "\n",
      "The least squares method minimizes the sum of the squared errors (SSE), where the error is the difference between the actual *y* value and the predicted *y* value (also called a *residual*). The formulas for calculating *β₀* and *β₁* are:\n",
      "\n",
      "*β₁ = Σ[(xᵢ - x̄)(yᵢ - ȳ)] / Σ[(xᵢ - x̄)²]*\n",
      "\n",
      "*β₀ = ȳ - β₁x̄*\n",
      "\n",
      "Where:\n",
      "\n",
      "*   *xᵢ* and *yᵢ* are individual data points.\n",
      "*   *x̄* is the mean of the *x* values.\n",
      "*   *ȳ* is the mean of the *y* values.\n",
      "*   Σ denotes summation.\n",
      "\n",
      "**Assumptions of Linear Regression**\n",
      "\n",
      "For linear regression to be valid and produce reliable results, several assumptions should be met:\n",
      "\n",
      "1.  **Linearity:**  The relationship between *x* and *y* is linear.  A scatter plot should show a roughly straight-line pattern.\n",
      "\n",
      "2.  **Independence:**  The errors (residuals) are independent of each other. This means that the error for one observation is not related to the error for another observation. This is especially important for time series data.\n",
      "\n",
      "3.  **Homoscedasticity (Constant Variance):** The variance of the errors is constant across all values of *x*. This means the spread of the data points around the regression line should be roughly the same for all values of *x*.\n",
      "\n",
      "4.  **Normality:** The errors are normally distributed. This assumption is mainly important for hypothesis testing and confidence intervals.\n",
      "\n",
      "**Evaluating Regression Models**\n",
      "\n",
      "Several metrics are used to evaluate the performance of linear regression models:\n",
      "\n",
      "1.  **Mean Squared Error (MSE):** The average of the squared differences between the predicted and actual values.\n",
      "    *   MSE = Σ(yᵢ - ŷᵢ)² / n\n",
      "    *   Lower MSE indicates a better fit.\n",
      "\n",
      "2.  **Root Mean Squared Error (RMSE):** The square root of the MSE.  RMSE is easier to interpret than MSE because it is in the same units as the dependent variable.\n",
      "    *   RMSE = √(MSE)\n",
      "    *   Lower RMSE indicates a better fit.\n",
      "\n",
      "3.  **R-squared (Coefficient of Determination):** Represents the proportion of variance in the dependent variable that can be predicted from the independent variable(s).  Ranges from 0 to 1.\n",
      "    *   R² = 1 - (SSE / SST)\n",
      "    *   Where SSE is the sum of squared errors (as defined above), and SST is the total sum of squares (the sum of squared differences between the actual *y* values and the mean of *y*).\n",
      "    *   Higher R-squared generally indicates a better fit, but it's important to consider adjusted R-squared when comparing models with different numbers of predictors (not applicable to simple linear regression).\n",
      "\n",
      "**Regularization (L1 and L2)**\n",
      "\n",
      "Regularization techniques are used to prevent overfitting, especially when dealing with multiple predictors (multiple linear regression, covered in a separate lesson) or complex relationships.  Overfitting occurs when a model learns the training data too well, including noise, and performs poorly on new, unseen data.  While not directly applicable to *simple* linear regression, understanding the concepts is important.\n",
      "\n",
      "1.  **L1 Regularization (Lasso Regression):** Adds a penalty term to the cost function (the function that the model tries to minimize) proportional to the *absolute value* of the coefficients.  This can force some coefficients to become exactly zero, effectively performing feature selection.\n",
      "\n",
      "2.  **L2 Regularization (Ridge Regression):** Adds a penalty term to the cost function proportional to the *square* of the coefficients.  This shrinks the coefficients towards zero but rarely forces them to be exactly zero.\n",
      "\n",
      "**Python Implementation with Scikit-learn**\n",
      "\n",
      "Scikit-learn provides a simple and efficient way to implement linear regression in Python.\n",
      "\n",
      "```python\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "# Sample data (replace with your own data)\n",
      "data = {'X': [1, 2, 3, 4, 5], 'Y': [2, 4, 5, 4, 5]}\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "X = df[['X']] # Independent variable (must be a 2D array)\n",
      "Y = df['Y']   # Dependent variable\n",
      "\n",
      "# Split data into training and testing sets\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42) # Adjust test_size and random_state as needed\n",
      "\n",
      "# Create a linear regression model\n",
      "model = LinearRegression()\n",
      "\n",
      "# Fit the model to the training data\n",
      "model.fit(X_train, Y_train)\n",
      "\n",
      "# Make predictions on the test data\n",
      "Y_pred = model.predict(X_test)\n",
      "\n",
      "# Evaluate the model\n",
      "mse = mean_squared_error(Y_test, Y_pred)\n",
      "rmse = np.sqrt(mse)\n",
      "r2 = r2_score(Y_test, Y_pred)\n",
      "\n",
      "print(f\"Mean Squared Error: {mse}\")\n",
      "print(f\"Root Mean Squared Error: {rmse}\")\n",
      "print(f\"R-squared: {r2}\")\n",
      "\n",
      "# Access the coefficients\n",
      "print(f\"Intercept: {model.intercept_}\")\n",
      "print(f\"Slope: {model.coef_[0]}\") #model.coef_ is an array; access the first element for simple linear regression\n",
      "\n",
      "```\n",
      "\n",
      "**Examples:**\n",
      "\n",
      "**Example 1: Predicting House Prices**\n",
      "\n",
      "Suppose we have data on the size (in square feet) and price of houses. Let's build a simple linear regression model to predict the price of a house based on its size.\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "import numpy as np\n",
      "\n",
      "# Sample Data\n",
      "data = {'Size': [1000, 1500, 2000, 2500, 3000],\n",
      "        'Price': [200000, 300000, 400000, 500000, 600000]}\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Prepare Data\n",
      "X = df[['Size']]\n",
      "y = df['Price']\n",
      "\n",
      "#Split data\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
      "\n",
      "# Build and Train Model\n",
      "model = LinearRegression()\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "# Make Predictions\n",
      "y_pred = model.predict(X_test)\n",
      "\n",
      "# Evaluate Model\n",
      "mse = mean_squared_error(y_test, y_pred)\n",
      "r2 = r2_score(y_test, y_pred)\n",
      "\n",
      "print(\"House Price Prediction Model:\")\n",
      "print(f\"MSE: {mse}\")\n",
      "print(f\"R-squared: {r2}\")\n",
      "print(f\"Intercept: {model.intercept_}\")\n",
      "print(f\"Coefficient: {model.coef_[0]}\")\n",
      "\n",
      "# Predict the price of a 1750 sq ft house\n",
      "new_size = [[1750]]  # Must be a 2D array\n",
      "predicted_price = model.predict(new_size)[0] # Access the first element of the returned array\n",
      "print(f\"Predicted price for a 1750 sq ft house: ${predicted_price:,.2f}\")  # Format with commas and 2 decimal places\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1.  We create a Pandas DataFrame with house size and price data.\n",
      "2.  We prepare the data by separating the independent variable (size) and dependent variable (price).\n",
      "3.  The dataset is split into training and testing sets.\n",
      "4.  We create a `LinearRegression` object, fit it to the training data, and make predictions on the test data.\n",
      "5.  We evaluate the model using MSE and R-squared.\n",
      "6.  We access the intercept and coefficient (slope) of the regression line.\n",
      "7.  We predict the price for a new house size. Note the use of `[[1750]]` to create a 2D array as required by scikit-learn.\n",
      "\n",
      "**Example 2: Simple Linear Regression on Salary vs. Experience**\n",
      "\n",
      "Let's assume we have data on years of experience and corresponding salaries.\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "import numpy as np\n",
      "\n",
      "# Sample data\n",
      "data = {'Experience': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
      "        'Salary': [30000, 40000, 50000, 60000, 75000, 80000, 90000, 100000, 110000, 120000]}\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Prepare data\n",
      "X = df[['Experience']]\n",
      "y = df['Salary']\n",
      "\n",
      "# Split data into training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
      "\n",
      "# Create and train the model\n",
      "model = LinearRegression()\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "# Make predictions\n",
      "y_pred = model.predict(X_test)\n",
      "\n",
      "# Evaluate the model\n",
      "mse = mean_squared_error(y_test, y_pred)\n",
      "r2 = r2_score(y_test, y_pred)\n",
      "\n",
      "print(\"Salary vs. Experience Model:\")\n",
      "print(f\"MSE: {mse}\")\n",
      "print(f\"R-squared: {r2}\")\n",
      "print(f\"Intercept: {model.intercept_}\")\n",
      "print(f\"Coefficient: {model.coef_[0]}\")\n",
      "\n",
      "# Predict salary for 12 years of experience\n",
      "new_experience = [[12]] # Must be a 2D array\n",
      "predicted_salary = model.predict(new_experience)[0]\n",
      "print(f\"Predicted salary for 12 years of experience: ${predicted_salary:,.2f}\")\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "This example is similar to the house price example, but it uses years of experience to predict salary. The code follows the same steps: creating data, preparing the data, splitting it into training and testing sets, training the model, making predictions, and evaluating the model.\n",
      "\n",
      "**Exercises:**\n",
      "\n",
      "**Exercise 1: Height vs. Weight**\n",
      "\n",
      "You are given the following data for 10 individuals:\n",
      "\n",
      "```\n",
      "Height (inches): [60, 62, 64, 65, 66, 67, 68, 70, 72, 74]\n",
      "Weight (pounds): [120, 130, 140, 145, 150, 155, 160, 170, 180, 190]\n",
      "```\n",
      "\n",
      "1.  Create a Pandas DataFrame with this data.\n",
      "2.  Build a simple linear regression model to predict weight based on height.\n",
      "3.  Print the MSE, RMSE, and R-squared of the model.\n",
      "4.  Print the intercept and slope of the regression line.\n",
      "5.  Predict the weight of a person who is 69 inches tall.\n",
      "\n",
      "**Exercise 2: Advertising Spending vs. Sales**\n",
      "\n",
      "You have data on advertising spending (in thousands of dollars) and the corresponding sales (in thousands of dollars):\n",
      "\n",
      "```\n",
      "Advertising: [10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
      "Sales: [50, 60, 70, 80, 90, 100, 110, 120, 130]\n",
      "```\n",
      "\n",
      "1.  Create a Pandas DataFrame.\n",
      "2.  Create a simple linear regression model to predict sales based on advertising spending.\n",
      "3.  Split the data into 80% training and 20% testing.\n",
      "4.  Evaluate the model.\n",
      "5.  Predict sales when advertising spending is $55,000.\n",
      "\n",
      "**Exercise 3: Explore the Impact of Train/Test Split**\n",
      "\n",
      "Using the \"Salary vs. Experience\" example code, modify the `test_size` parameter in the `train_test_split` function to different values (e.g., 0.1, 0.5, 0.9). Observe how the MSE and R-squared values change. What does this tell you about the importance of the training/testing split?\n",
      "\n",
      "**Assessments:**\n",
      "\n",
      "The following are designed as auto-gradable assessments.  Assume a function is provided that takes the inputs and returns a single value (e.g., the predicted value, the MSE).\n",
      "\n",
      "**Question 1:**\n",
      "\n",
      "Given the following data: `X = [1, 2, 3, 4, 5]` and `Y = [2, 4, 6, 8, 10]`.  What is the predicted value of Y for X = 6, using a simple linear regression model trained on this data? (Provide the *numerical* predicted value as a float.)\n",
      "\n",
      "**Question 2:**\n",
      "\n",
      "Using the same data as Question 1, what is the Mean Squared Error (MSE) of a simple linear regression model trained and evaluated on the *same* data? (Provide the MSE value as a float, rounded to 2 decimal places.)\n",
      "\n",
      "**Question 3:**\n",
      "\n",
      "You train a simple linear regression model and find the intercept is 1.0 and the slope is 2.0. Given a data point X = 3, what is the predicted value of Y? (Provide the predicted Y value as a float.)\n",
      "\n",
      "**Question 4:**\n",
      "\n",
      "Given data points `X = [1, 2, 3]` and `Y = [2, 3, 5]`, calculate the R-squared value for a simple linear regression model. Round the R-squared value to 2 decimal places. (Provide the R-squared value as a float.)\n",
      "\n",
      "**Question 5:**\n",
      "\n",
      "Which of the following is NOT a common assumption of linear regression?\n",
      "a) Linearity\n",
      "b) Independence of Errors\n",
      "c) Multicollinearity\n",
      "d) Homoscedasticity\n",
      "\n",
      "(Provide the letter corresponding to the correct answer as a string, e.g., \"c\")\n",
      "\n",
      "**Additional Resources:**\n",
      "\n",
      "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
      "- https://www.statsmodels.org/stable/index.html\n",
      "- https://realpython.com/linear-regression-in-python/\n",
      "- https://towardsdatascience.com/linear-regression-using-python-b0c96e1a243f\n",
      "- https://machinelearningmastery.com/linear-regression-in-python/\n",
      "- https://www.coursera.org/learn/machine-learning (Andrew Ng's Machine Learning Course, Coursera)\n",
      "\n",
      "\n",
      "\n",
      "=== UNIT 2: Supervised Learning: Regression ===\n",
      "\n",
      "Okay, here is the detailed lesson content for 'Multiple Linear Regression', designed to meet the specified learning objectives and requirements.\n",
      "\n",
      "**Lesson Title: Multiple Linear Regression**\n",
      "\n",
      "**Learning Objectives:**\n",
      "\n",
      "*   Understand the principles of linear regression.\n",
      "*   Implement linear regression using Python libraries (e.g., Scikit-learn).\n",
      "*   Evaluate regression models using appropriate metrics (MSE, RMSE, R-squared).\n",
      "*   Learn about regularization techniques to prevent overfitting (L1, L2 regularization).\n",
      "\n",
      "**Readings:**\n",
      "\n",
      "**1. Introduction to Linear Regression**\n",
      "\n",
      "Linear regression is a statistical method used to model the relationship between a dependent variable (also known as the response variable or target variable) and one or more independent variables (also known as predictor variables or features).  The goal is to find the best-fitting linear equation to describe this relationship.\n",
      "\n",
      "*   **Simple Linear Regression:**  Deals with one independent variable. The equation is:  `y = β0 + β1*x + ε`, where:\n",
      "    *   `y` is the dependent variable.\n",
      "    *   `x` is the independent variable.\n",
      "    *   `β0` is the y-intercept (the value of y when x is 0).\n",
      "    *   `β1` is the slope (the change in y for a one-unit change in x).\n",
      "    *   `ε` is the error term (representing the difference between the observed and predicted values).\n",
      "\n",
      "*   **Multiple Linear Regression:**  Deals with two or more independent variables. The equation is: `y = β0 + β1*x1 + β2*x2 + ... + βn*xn + ε`, where:\n",
      "    *   `y` is the dependent variable.\n",
      "    *   `x1, x2, ..., xn` are the independent variables.\n",
      "    *   `β0` is the y-intercept.\n",
      "    *   `β1, β2, ..., βn` are the coefficients for each independent variable.\n",
      "    *   `ε` is the error term.\n",
      "\n",
      "The coefficients (β values) are estimated using the Ordinary Least Squares (OLS) method, which minimizes the sum of the squared differences between the observed and predicted values of the dependent variable.\n",
      "\n",
      "**2. Implementing Linear Regression with Scikit-learn**\n",
      "\n",
      "Scikit-learn is a powerful Python library for machine learning.  It provides a simple and efficient way to implement linear regression.\n",
      "\n",
      "*   **Steps:**\n",
      "\n",
      "    1.  **Import necessary libraries:** `import numpy as np`, `import pandas as pd`, `from sklearn.linear_model import LinearRegression`, `from sklearn.model_selection import train_test_split`, `from sklearn.metrics import mean_squared_error, r2_score`\n",
      "    2.  **Load your data:**  Read your data from a CSV file or other source into a Pandas DataFrame.\n",
      "    3.  **Prepare your data:**  Identify the independent variables (features) and the dependent variable (target). Separate them into `X` and `y` variables.\n",
      "    4.  **Split the data:** Split your data into training and testing sets using `train_test_split(X, y, test_size=0.2, random_state=42)`.  This creates `X_train`, `X_test`, `y_train`, and `y_test` variables.\n",
      "    5.  **Create and train the model:**  Create a `LinearRegression()` object and train it on the training data using `model.fit(X_train, y_train)`.\n",
      "    6.  **Make predictions:** Use the trained model to make predictions on the test data using `y_pred = model.predict(X_test)`.\n",
      "\n",
      "**3. Model Evaluation Metrics**\n",
      "\n",
      "Evaluating the performance of your linear regression model is crucial.  Common metrics include:\n",
      "\n",
      "*   **Mean Squared Error (MSE):**  The average of the squared differences between the predicted and actual values.  A lower MSE indicates better performance.\n",
      "    *   Formula:  `MSE = (1/n) * Σ(y_i - ŷ_i)^2`, where `y_i` is the actual value, `ŷ_i` is the predicted value, and `n` is the number of data points.\n",
      "\n",
      "*   **Root Mean Squared Error (RMSE):** The square root of the MSE. It provides a more interpretable measure of error as it is in the same units as the dependent variable.\n",
      "    *   Formula: `RMSE = √MSE`\n",
      "\n",
      "*   **R-squared (Coefficient of Determination):**  Represents the proportion of the variance in the dependent variable that is explained by the independent variables.  Ranges from 0 to 1.  A higher R-squared indicates a better fit.\n",
      "    *   Formula: `R² = 1 - (SS_res / SS_tot)`, where `SS_res` is the sum of squares of residuals and `SS_tot` is the total sum of squares.\n",
      "\n",
      "**4. Regularization Techniques for Overfitting**\n",
      "\n",
      "Overfitting occurs when a model learns the training data too well and performs poorly on unseen data. Regularization techniques help prevent overfitting by adding a penalty term to the model's cost function, which discourages overly complex models.\n",
      "\n",
      "*   **L1 Regularization (Lasso Regression):** Adds the absolute value of the coefficients to the cost function. This can lead to feature selection, as some coefficients may be driven to zero, effectively removing those features from the model.\n",
      "\n",
      "*   **L2 Regularization (Ridge Regression):**  Adds the squared value of the coefficients to the cost function.  This shrinks the coefficients towards zero, but typically doesn't eliminate them entirely.\n",
      "\n",
      "*   **Elastic Net Regularization:** A combination of L1 and L2 regularization.  It provides a balance between feature selection and coefficient shrinkage.\n",
      "\n",
      "*   **Implementation in Scikit-learn:**\n",
      "\n",
      "    *   **Lasso:** `from sklearn.linear_model import Lasso; lasso = Lasso(alpha=0.1); lasso.fit(X_train, y_train)`\n",
      "    *   **Ridge:** `from sklearn.linear_model import Ridge; ridge = Ridge(alpha=0.1); ridge.fit(X_train, y_train)`\n",
      "    *   **ElasticNet:** `from sklearn.linear_model import ElasticNet; elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5); elastic_net.fit(X_train, y_train)`\n",
      "\n",
      "    The `alpha` parameter controls the strength of the regularization. A higher `alpha` value indicates stronger regularization.  The `l1_ratio` parameter in ElasticNet controls the balance between L1 and L2 regularization (0 for L2, 1 for L1).\n",
      "\n",
      "**Examples:**\n",
      "\n",
      "**1. Simple Linear Regression Example (House Price Prediction based on Size)**\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "\n",
      "# Sample data (replace with your actual data)\n",
      "data = {'Size': [1000, 1500, 1200, 1800, 2000],\n",
      "        'Price': [200000, 300000, 250000, 350000, 400000]}\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Prepare the data\n",
      "X = df[['Size']]  # Independent variable (feature)\n",
      "y = df['Price']   # Dependent variable (target)\n",
      "\n",
      "# Split the data into training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Create and train the model\n",
      "model = LinearRegression()\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "# Make predictions\n",
      "y_pred = model.predict(X_test)\n",
      "\n",
      "# Evaluate the model\n",
      "mse = mean_squared_error(y_test, y_pred)\n",
      "rmse = np.sqrt(mse)\n",
      "r2 = r2_score(y_test, y_pred)\n",
      "\n",
      "print(f\"MSE: {mse}\")\n",
      "print(f\"RMSE: {rmse}\")\n",
      "print(f\"R-squared: {r2}\")\n",
      "\n",
      "# Print the model coefficients\n",
      "print(f\"Intercept: {model.intercept_}\")\n",
      "print(f\"Coefficient (Slope): {model.coef_[0]}\") # accessing the first element since X has only 1 feature\n",
      "\n",
      "```\n",
      "\n",
      "**Explanation:** This example demonstrates how to use Scikit-learn to perform simple linear regression. It loads sample data, splits it into training and testing sets, creates and trains a `LinearRegression` model, makes predictions, and evaluates the model using MSE, RMSE, and R-squared. It also prints the intercept and coefficient of the regression line.\n",
      "\n",
      "**2. Multiple Linear Regression Example (House Price Prediction based on Size and Number of Bedrooms)**\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "\n",
      "# Sample data (replace with your actual data)\n",
      "data = {'Size': [1000, 1500, 1200, 1800, 2000],\n",
      "        'Bedrooms': [2, 3, 2, 4, 3],\n",
      "        'Price': [200000, 300000, 250000, 350000, 400000]}\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Prepare the data\n",
      "X = df[['Size', 'Bedrooms']]  # Independent variables (features)\n",
      "y = df['Price']   # Dependent variable (target)\n",
      "\n",
      "# Split the data into training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Create and train the model\n",
      "model = LinearRegression()\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "# Make predictions\n",
      "y_pred = model.predict(X_test)\n",
      "\n",
      "# Evaluate the model\n",
      "mse = mean_squared_error(y_test, y_pred)\n",
      "rmse = np.sqrt(mse)\n",
      "r2 = r2_score(y_test, y_pred)\n",
      "\n",
      "print(f\"MSE: {mse}\")\n",
      "print(f\"RMSE: {rmse}\")\n",
      "print(f\"R-squared: {r2}\")\n",
      "\n",
      "# Print the model coefficients\n",
      "print(f\"Intercept: {model.intercept_}\")\n",
      "print(f\"Coefficients: {model.coef_}\") # Accessing the elements since X has multiple features (Size and Bedrooms)\n",
      "```\n",
      "\n",
      "**Explanation:** This example extends the previous one to include multiple independent variables (Size and Bedrooms).  The rest of the code remains similar, demonstrating how easily Scikit-learn handles multiple linear regression.\n",
      "\n",
      "**3. Regularization Example (Ridge Regression)**\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Sample data (replace with your actual data) - including more features and data\n",
      "data = {'Feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
      "        'Feature2': [11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n",
      "        'Feature3': [21, 22, 23, 24, 25, 26, 27, 28, 29, 30],\n",
      "        'Target': [31, 33, 35, 37, 39, 41, 43, 45, 47, 49]}\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Prepare the data\n",
      "X = df[['Feature1', 'Feature2', 'Feature3']]\n",
      "y = df['Target']\n",
      "\n",
      "# Split the data into training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Standardize the features (important for regularization)\n",
      "scaler = StandardScaler()\n",
      "X_train = scaler.fit_transform(X_train)\n",
      "X_test = scaler.transform(X_test)\n",
      "\n",
      "\n",
      "# Create and train the Ridge regression model\n",
      "ridge = Ridge(alpha=1.0)  # alpha is the regularization strength\n",
      "ridge.fit(X_train, y_train)\n",
      "\n",
      "# Make predictions\n",
      "y_pred = ridge.predict(X_test)\n",
      "\n",
      "# Evaluate the model\n",
      "mse = mean_squared_error(y_test, y_pred)\n",
      "rmse = np.sqrt(mse)\n",
      "r2 = r2_score(y_test, y_pred)\n",
      "\n",
      "print(f\"MSE: {mse}\")\n",
      "print(f\"RMSE: {rmse}\")\n",
      "print(f\"R-squared: {r2}\")\n",
      "\n",
      "print(f\"Intercept: {ridge.intercept_}\")\n",
      "print(f\"Coefficients: {ridge.coef_}\")\n",
      "```\n",
      "\n",
      "**Explanation:** This example demonstrates Ridge regression.  Crucially, it includes `StandardScaler` to standardize the features before applying regularization.  Regularization is more effective when features are on the same scale. The `alpha` parameter controls the strength of the regularization.\n",
      "**Exercises:**\n",
      "\n",
      "**1. Data Exploration and Preparation:**\n",
      "\n",
      "*   Download a dataset of your choice (e.g., from Kaggle) that is suitable for regression. Examples include:\n",
      "    *   House Prices dataset\n",
      "    *   Medical Cost Personal Datasets\n",
      "*   Load the data into a Pandas DataFrame.\n",
      "*   Explore the data using `.head()`, `.describe()`, and `.info()`.\n",
      "*   Identify the independent and dependent variables.\n",
      "*   Handle any missing values using appropriate methods (e.g., imputation or removal).\n",
      "*   Encode categorical variables (if any) using one-hot encoding or label encoding.\n",
      "\n",
      "**2. Linear Regression Implementation:**\n",
      "\n",
      "*   Using the prepared data from Exercise 1, split the data into training and testing sets.\n",
      "*   Create a `LinearRegression` model.\n",
      "*   Train the model on the training data.\n",
      "*   Make predictions on the test data.\n",
      "*   Evaluate the model using MSE, RMSE, and R-squared.\n",
      "*   Print the coefficients of the model.\n",
      "\n",
      "**3. Regularization Practice:**\n",
      "\n",
      "*   Using the same dataset and setup from Exercise 2, implement Ridge, Lasso, and Elastic Net regression.\n",
      "*   Experiment with different values of the `alpha` parameter for each regularization technique.\n",
      "*   Compare the performance of the regularized models with the unregularized linear regression model based on the evaluation metrics (MSE, RMSE, R-squared).\n",
      "*   Analyze the coefficients of the regularized models.  Which features are most important?  How do the coefficients change as you increase the regularization strength?\n",
      "\n",
      "**4. Feature Selection and Model Comparison:**\n",
      "\n",
      "*   Using a dataset with a large number of features, use Lasso regression to perform feature selection.\n",
      "*   Train a Linear Regression model using only the features selected by Lasso.\n",
      "*   Compare the performance of this model with a Linear Regression model trained on all the original features.\n",
      "*   Discuss the trade-offs between model complexity and performance.\n",
      "\n",
      "**Assessments:**\n",
      "\n",
      "**(Each question is worth 2 points.  Total: 10 points)**\n",
      "\n",
      "**Instructions:** Write Python code to answer each question.  The code should be runnable and produce the correct output.  Assume you have the following data loaded into a Pandas DataFrame called `df`:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Sample data (replace with your actual data)\n",
      "data = {'Feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
      "        'Feature2': [11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n",
      "        'Feature3': [21, 22, 23, 24, 25, 26, 27, 28, 29, 30],\n",
      "        'Target': [31, 33, 35, 37, 39, 41, 43, 45, 47, 49]}\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Prepare the data\n",
      "X = df[['Feature1', 'Feature2', 'Feature3']]\n",
      "y = df['Target']\n",
      "\n",
      "# Split the data into training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Standardize the features (important for regularization)\n",
      "scaler = StandardScaler()\n",
      "X_train = scaler.fit_transform(X_train)\n",
      "X_test = scaler.transform(X_test)\n",
      "\n",
      "```\n",
      "\n",
      "**1. Linear Regression Coefficients:**\n",
      "\n",
      "Train a `LinearRegression` model on `X_train` and `y_train`.  What is the value of the coefficient for 'Feature2'? Print the value.\n",
      "\n",
      "```python\n",
      "model = LinearRegression()\n",
      "model.fit(X_train, y_train)\n",
      "print(model.coef_[1])\n",
      "```\n",
      "\n",
      "**2.  Ridge Regression R-squared:**\n",
      "\n",
      "Train a `Ridge` regression model with `alpha=0.5` on `X_train` and `y_train`.  What is the R-squared score on the `X_test` and `y_test` sets? Print the value.\n",
      "\n",
      "```python\n",
      "ridge = Ridge(alpha=0.5)\n",
      "ridge.fit(X_train, y_train)\n",
      "y_pred_ridge = ridge.predict(X_test)\n",
      "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
      "print(r2_ridge)\n",
      "```\n",
      "\n",
      "**3.  Lasso Regression MSE:**\n",
      "\n",
      "Train a `Lasso` regression model with `alpha=0.1` on `X_train` and `y_train`. What is the Mean Squared Error (MSE) on the `X_test` and `y_test` sets? Print the value.\n",
      "\n",
      "```python\n",
      "lasso = Lasso(alpha=0.1)\n",
      "lasso.fit(X_train, y_train)\n",
      "y_pred_lasso = lasso.predict(X_test)\n",
      "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
      "print(mse_lasso)\n",
      "```\n",
      "\n",
      "**4.  Impact of Alpha on Ridge Coefficients:**\n",
      "\n",
      "Train two `Ridge` regression models. The first with `alpha=0.1` and the second with `alpha=10`. Which model has larger coefficient values on average (absolute values) for `Feature1`, `Feature2` and `Feature3`? Print 'Model 1' or 'Model 2'.\n",
      "\n",
      "```python\n",
      "ridge1 = Ridge(alpha=0.1)\n",
      "ridge1.fit(X_train, y_train)\n",
      "ridge2 = Ridge(alpha=10)\n",
      "ridge2.fit(X_train, y_train)\n",
      "\n",
      "avg_coef1 = np.mean(np.abs(ridge1.coef_))\n",
      "avg_coef2 = np.mean(np.abs(ridge2.coef_))\n",
      "\n",
      "if avg_coef1 > avg_coef2:\n",
      "    print(\"Model 1\")\n",
      "else:\n",
      "    print(\"Model 2\")\n",
      "```\n",
      "\n",
      "**5. Simple Linear Regression MSE:**\n",
      "\n",
      "Create two `Feature` variables by only choosing `Feature1` and `Feature2` from `df`. Train a Simple Linear Regression model on `X_train` and `y_train`. What is the Mean Squared Error (MSE) on the `X_test` and `y_test` sets? Print the value.\n",
      "\n",
      "```python\n",
      "X_simple = df[['Feature1', 'Feature2']]\n",
      "X_train_simple, X_test_simple, y_train, y_test = train_test_split(X_simple, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Standardize the features (important for regularization)\n",
      "scaler = StandardScaler()\n",
      "X_train_simple = scaler.fit_transform(X_train_simple)\n",
      "X_test_simple = scaler.transform(X_test_simple)\n",
      "\n",
      "model_simple = LinearRegression()\n",
      "model_simple.fit(X_train_simple, y_train)\n",
      "y_pred_simple = model_simple.predict(X_test_simple)\n",
      "mse_simple = mean_squared_error(y_test, y_pred_simple)\n",
      "print(mse_simple)\n",
      "```\n",
      "\n",
      "**Additional Resources:**\n",
      "\n",
      "- https://scikit-learn.org/stable/modules/linear_model.html\n",
      "- https://www.statsmodels.org/stable/index.html\n",
      "- https://towardsdatascience.com/linear-regression-using-python-b6e61f0a02e5\n",
      "- https://machinelearningmastery.com/linear-regression-for-machine-learning/\n",
      "- https://www.kaggle.com/learn/machine-learning\n",
      "- https://realpython.com/linear-regression-in-python/\n",
      "\n",
      "Error generating content for lesson Model Evaluation and Regularization: 'NoneType' object has no attribute 'splitlines'\n",
      "Error generating content for lesson Logistic Regression: 'NoneType' object has no attribute 'splitlines'\n",
      "Error generating content for lesson Support Vector Machines: 'NoneType' object has no attribute 'splitlines'\n",
      "Error generating content for lesson Model Evaluation and Imbalanced Data: 'NoneType' object has no attribute 'splitlines'\n",
      "\n",
      "\n",
      "=== UNIT 4: Unsupervised Learning: Clustering ===\n",
      "\n",
      "```plain\n",
      "## Lesson: K-Means Clustering\n",
      "\n",
      "**Learning Objectives:**\n",
      "\n",
      "*   Understand the principles of k-means clustering.\n",
      "*   Implement k-means clustering using Python.\n",
      "*   Evaluate clustering results using appropriate metrics (Silhouette score).\n",
      "*   Learn about techniques for determining the optimal number of clusters.\n",
      "\n",
      "**Readings:**\n",
      "\n",
      "**1. Introduction to K-Means Clustering:**\n",
      "\n",
      "K-means clustering is an unsupervised learning algorithm used to group data points into clusters based on their similarity. The algorithm aims to partition n data points into k clusters, where each data point belongs to the cluster with the nearest mean (centroid).  \"Unsupervised\" means the algorithm learns patterns without labeled data.\n",
      "\n",
      "**Key Concepts:**\n",
      "\n",
      "*   **Clusters:** Groups of similar data points.\n",
      "*   **Centroid:** The center of a cluster, typically calculated as the mean of the data points within that cluster.\n",
      "*   **Distance Metric:** A function used to measure the similarity or dissimilarity between data points. Common distance metrics include Euclidean distance, Manhattan distance, and Cosine similarity. Euclidean distance is the most commonly used and is defined as:  sqrt(sum((xi - yi)^2)) for points x and y with n dimensions.\n",
      "*   **Iteration:** A single pass through the data where cluster assignments and centroids are updated.\n",
      "\n",
      "**Algorithm Steps:**\n",
      "\n",
      "1.  **Initialization:** Randomly select k initial centroids.\n",
      "2.  **Assignment:** Assign each data point to the nearest centroid based on the chosen distance metric.\n",
      "3.  **Update:** Recalculate the centroids by taking the mean of all data points assigned to that cluster.\n",
      "4.  **Iteration:** Repeat steps 2 and 3 until the centroids no longer change significantly or a maximum number of iterations is reached.  This is referred to as convergence.\n",
      "\n",
      "**2. Implementing K-Means in Python (using scikit-learn):**\n",
      "\n",
      "The scikit-learn library provides a convenient implementation of the k-means algorithm.\n",
      "\n",
      "```python\n",
      "from sklearn.cluster import KMeans\n",
      "import numpy as np\n",
      "\n",
      "# Sample data (replace with your actual data)\n",
      "X = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])\n",
      "\n",
      "# Create a K-Means object with the desired number of clusters (k=2)\n",
      "kmeans = KMeans(n_clusters=2, random_state=0, n_init='auto') #n_init warns us to explicitly set this value; it sets the number of times the algorithm will be run with different centroid seeds.\n",
      "\n",
      "# Fit the model to the data\n",
      "kmeans.fit(X)\n",
      "\n",
      "# Get the cluster labels for each data point\n",
      "labels = kmeans.labels_\n",
      "\n",
      "# Get the coordinates of the cluster centroids\n",
      "centroids = kmeans.cluster_centers_\n",
      "\n",
      "print(\"Cluster labels:\", labels)\n",
      "print(\"Centroids:\", centroids)\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "*   `KMeans(n_clusters=2, random_state=0, n_init='auto')`: Creates a K-Means object with 2 clusters. `random_state` ensures reproducibility, and `n_init` specifies the number of times k-means will be run with different centroid seeds.\n",
      "*   `kmeans.fit(X)`: Fits the model to the data X, performing the clustering process.\n",
      "*   `kmeans.labels_`:  Returns an array containing the cluster assignment for each data point (0 or 1 in this example).\n",
      "*   `kmeans.cluster_centers_`: Returns an array containing the coordinates of the cluster centroids.\n",
      "\n",
      "**3. Evaluating Clustering Performance (Silhouette Score):**\n",
      "\n",
      "The Silhouette score is a metric used to evaluate the quality of clustering. It measures how well each data point fits within its assigned cluster compared to other clusters. The Silhouette score ranges from -1 to 1:\n",
      "\n",
      "*   **+1:** Indicates that the data point is well-clustered and far away from other clusters.\n",
      "*   **0:** Indicates that the data point is close to the decision boundary between two clusters.\n",
      "*   **-1:** Indicates that the data point might be assigned to the wrong cluster.\n",
      "\n",
      "The average Silhouette score across all data points provides an overall measure of the clustering quality.  Higher average Silhouette scores indicate better clustering.\n",
      "\n",
      "```python\n",
      "from sklearn.metrics import silhouette_score\n",
      "\n",
      "# Assuming 'X' is your data and 'labels' are the cluster assignments from K-Means\n",
      "silhouette_avg = silhouette_score(X, labels)\n",
      "print(\"Silhouette Score:\", silhouette_avg)\n",
      "```\n",
      "\n",
      "**4. Determining the Optimal Number of Clusters (Elbow Method and Silhouette Analysis):**\n",
      "\n",
      "Choosing the correct number of clusters (k) is crucial for k-means clustering. Two common methods for determining the optimal k are:\n",
      "\n",
      "*   **Elbow Method:**  Plot the within-cluster sum of squares (WCSS) for different values of k. WCSS is the sum of the squared distances between each data point and its centroid. As k increases, WCSS decreases. The \"elbow point\" on the plot, where the rate of decrease slows down significantly, is often considered the optimal number of clusters.  The elbow point represents a trade-off between minimizing WCSS and avoiding overfitting.\n",
      "\n",
      "*   **Silhouette Analysis:**  Calculate the average Silhouette score for different values of k.  Plot the Silhouette scores against the corresponding values of k. The value of k that yields the highest Silhouette score is generally considered the optimal number of clusters.\n",
      "\n",
      "```python\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.metrics import silhouette_score\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Sample data (replace with your actual data)\n",
      "X = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])\n",
      "\n",
      "# Elbow Method\n",
      "wcss = []\n",
      "for i in range(1, 5):\n",
      "    kmeans = KMeans(n_clusters=i, random_state=0, n_init='auto')\n",
      "    kmeans.fit(X)\n",
      "    wcss.append(kmeans.inertia_) #Inertia: Sum of squared distances of samples to their closest cluster center.\n",
      "plt.plot(range(1, 5), wcss)\n",
      "plt.title('Elbow Method')\n",
      "plt.xlabel('Number of clusters')\n",
      "plt.ylabel('WCSS')\n",
      "plt.show()\n",
      "\n",
      "# Silhouette Analysis\n",
      "silhouette_scores = []\n",
      "for i in range(2, 5): # Silhouette score requires at least 2 clusters\n",
      "    kmeans = KMeans(n_clusters=i, random_state=0, n_init='auto')\n",
      "    labels = kmeans.fit_predict(X)\n",
      "    silhouette_avg = silhouette_score(X, labels)\n",
      "    silhouette_scores.append(silhouette_avg)\n",
      "plt.plot(range(2, 5), silhouette_scores)\n",
      "plt.title('Silhouette Analysis')\n",
      "plt.xlabel('Number of clusters')\n",
      "plt.ylabel('Silhouette Score')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "**Examples:**\n",
      "\n",
      "**Example 1: Customer Segmentation**\n",
      "\n",
      "Imagine you have data on customer spending habits and demographics. You can use k-means to segment customers into different groups based on their purchasing behavior. This allows you to tailor marketing campaigns and product recommendations to specific customer segments.\n",
      "\n",
      "**Code (Illustrative):**\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Load customer data (replace with your actual data file)\n",
      "customer_data = pd.read_csv(\"customer_data.csv\") #Assume columns like 'Spending', 'Age', 'Income'\n",
      "\n",
      "# Preprocess the data (scaling is important for k-means)\n",
      "scaler = StandardScaler()\n",
      "scaled_data = scaler.fit_transform(customer_data)\n",
      "\n",
      "# Determine the optimal number of clusters (using elbow method or silhouette analysis - not shown here for brevity)\n",
      "k = 3  # Assuming 3 clusters is determined to be optimal\n",
      "\n",
      "# Apply K-Means clustering\n",
      "kmeans = KMeans(n_clusters=k, random_state=0, n_init='auto')\n",
      "kmeans.fit(scaled_data)\n",
      "customer_data['Cluster'] = kmeans.labels_\n",
      "\n",
      "# Analyze the clusters\n",
      "print(customer_data.groupby('Cluster').mean()) #See average spending, age, etc. for each cluster\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "*   Loads customer data from a CSV file.\n",
      "*   Scales the data using `StandardScaler` to ensure that all features have a similar range of values.  Scaling prevents features with larger values from dominating the clustering process.\n",
      "*   Applies K-Means clustering with the determined optimal number of clusters (k=3 in this example).\n",
      "*   Adds a 'Cluster' column to the dataframe indicating the cluster assignment for each customer.\n",
      "*   Groups the data by cluster and calculates the mean of each feature for each cluster, allowing you to understand the characteristics of each customer segment.\n",
      "\n",
      "**Example 2: Image Compression**\n",
      "\n",
      "K-means can be used to reduce the number of colors in an image, effectively compressing the image.  Each color is treated as a data point in RGB space.\n",
      "\n",
      "**Code (Illustrative):**\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from sklearn.cluster import KMeans\n",
      "from PIL import Image\n",
      "\n",
      "# Load the image\n",
      "image = Image.open(\"image.jpg\")\n",
      "image = np.array(image)\n",
      "\n",
      "# Reshape the image into a list of pixels\n",
      "pixels = image.reshape(-1, 3) #Reshape to (height*width, 3) - each row is an RGB tuple\n",
      "\n",
      "# Perform K-Means clustering\n",
      "k = 8  # Reduce to 8 colors\n",
      "kmeans = KMeans(n_clusters=k, random_state=0, n_init='auto')\n",
      "kmeans.fit(pixels)\n",
      "\n",
      "# Replace each pixel with its centroid color\n",
      "new_pixels = kmeans.cluster_centers_[kmeans.labels_]\n",
      "\n",
      "# Reshape back into an image\n",
      "new_image = new_pixels.reshape(image.shape).astype(np.uint8)\n",
      "new_image = Image.fromarray(new_image)\n",
      "new_image.save(\"compressed_image.jpg\")\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "*   Loads an image using the PIL (Pillow) library.\n",
      "*   Reshapes the image into a list of pixels, where each pixel is represented as an RGB tuple.\n",
      "*   Applies K-Means clustering to group similar colors together.\n",
      "*   Replaces each pixel's color with the color of its assigned centroid.  This reduces the number of unique colors in the image.\n",
      "*   Reshapes the data back into an image and saves the compressed image.\n",
      "\n",
      "**Exercises:**\n",
      "\n",
      "**Exercise 1: Data Generation and Clustering**\n",
      "\n",
      "1.  Generate a synthetic dataset with 200 data points using `sklearn.datasets.make_blobs` with `n_features=2`, `centers=3`, and `random_state=42`.\n",
      "2.  Apply K-Means clustering with `k=3` to the generated data.\n",
      "3.  Visualize the data points and the cluster centroids using matplotlib. Color the data points according to their cluster assignments.\n",
      "\n",
      "**Exercise 2: Optimal Number of Clusters**\n",
      "\n",
      "1.  Using the same dataset generated in Exercise 1, apply K-Means clustering for different values of k (from 2 to 6).\n",
      "2.  Calculate the Silhouette score for each value of k.\n",
      "3.  Plot the Silhouette scores against the corresponding values of k.\n",
      "4.  Determine the optimal number of clusters based on the Silhouette score plot.\n",
      "\n",
      "**Exercise 3:  K-Means on Iris Dataset**\n",
      "\n",
      "1. Load the Iris dataset from sklearn.datasets.\n",
      "2. Apply K-Means clustering to the dataset with k=3.\n",
      "3. Evaluate the clustering performance using the Silhouette Score.\n",
      "4. Print the Silhouette Score to the console.\n",
      "\n",
      "**Assessments:**\n",
      "\n",
      "**Assessment 1:**\n",
      "\n",
      "Which of the following statements about K-Means clustering is TRUE?\n",
      "\n",
      "(a) K-Means is a supervised learning algorithm.\n",
      "(b) K-Means requires labeled data.\n",
      "(c) K-Means aims to minimize the distance between data points and their assigned cluster centroids.\n",
      "(d) K-Means always finds the globally optimal clustering solution.\n",
      "\n",
      "*Answer: (c)*\n",
      "\n",
      "**Assessment 2:**\n",
      "\n",
      "What does the Silhouette score measure?\n",
      "\n",
      "(a) The accuracy of a classification model.\n",
      "(b) The sum of squared errors in a regression model.\n",
      "(c) The quality of a clustering, based on the separation and cohesion of clusters.\n",
      "(d) The complexity of a decision tree.\n",
      "\n",
      "*Answer: (c)*\n",
      "\n",
      "**Assessment 3:**\n",
      "\n",
      "What is the purpose of the \"elbow method\" in K-Means clustering?\n",
      "\n",
      "(a) To visualize the cluster assignments.\n",
      "(b) To determine the optimal number of clusters.\n",
      "(c) To calculate the distance between data points.\n",
      "(d) To initialize the cluster centroids.\n",
      "\n",
      "*Answer: (b)*\n",
      "\n",
      "**Assessment 4:**\n",
      "\n",
      "In the context of K-Means, what is a centroid?\n",
      "\n",
      "(a) A data point in the dataset.\n",
      "(b) The center of a cluster, calculated as the mean of its data points.\n",
      "(c) The boundary between two clusters.\n",
      "(d) A measure of the similarity between data points.\n",
      "\n",
      "*Answer: (b)*\n",
      "\n",
      "**Assessment 5:**\n",
      "\n",
      "Complete the following Python code to perform K-Means clustering on a dataset `X` with 4 clusters:\n",
      "\n",
      "```python\n",
      "from sklearn.cluster import KMeans\n",
      "import numpy as np\n",
      "\n",
      "X = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])\n",
      "\n",
      "kmeans = KMeans(n_clusters=4, random_state=0, n_init='auto')\n",
      "kmeans.fit(X)\n",
      "\n",
      "labels = kmeans.labels_\n",
      "print(labels)\n",
      "```\n",
      "\n",
      "The code is complete as is. The important part is that the student correctly sets `n_clusters=4`. The autograder would check if the code runs without errors and produces cluster labels.\n",
      "\n",
      "**Assessment 6:**\n",
      "\n",
      "True or False: Scaling your data before applying K-Means is always necessary.\n",
      "\n",
      "*Answer: False (It's often recommended, but not strictly *always* necessary. It's most important when features have vastly different scales.)*\n",
      "\n",
      "**Additional Resources:**\n",
      "\n",
      "- https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
      "- https://scikit-learn.org/stable/modules/clustering.html#k-means\n",
      "- https://realpython.com/kmeans-clustering-python/\n",
      "- https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/\n",
      "- https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluate-methods-51b7f60c4eb8\n",
      "- https://www.datacamp.com/tutorial/k-means-clustering-in-python\n",
      "```\n",
      "\n",
      "Error generating content for lesson Evaluating Clustering Performance: 'NoneType' object has no attribute 'splitlines'\n",
      "\n",
      "\n",
      "=== UNIT 4: Unsupervised Learning: Clustering ===\n",
      "\n",
      "Here's the lesson content on Hierarchical Clustering, designed to be comprehensive and engaging:\n",
      "\n",
      "**Lesson Title: Hierarchical Clustering**\n",
      "\n",
      "**Learning Objectives:**\n",
      "\n",
      "*   Understand the principles of hierarchical clustering (agglomerative and divisive).\n",
      "*   Implement hierarchical clustering using Python (scikit-learn).\n",
      "*   Visualize dendrograms and interpret hierarchical clustering results.\n",
      "*   Learn about linkage methods and their impact on cluster formation.\n",
      "*   Compare and contrast hierarchical clustering with k-means clustering.\n",
      "\n",
      "**Readings:**\n",
      "\n",
      "**1. Introduction to Hierarchical Clustering:**\n",
      "\n",
      "Hierarchical clustering is a method of cluster analysis which seeks to build a hierarchy of clusters. Unlike k-means, it doesn't require specifying the number of clusters beforehand. There are primarily two types:\n",
      "\n",
      "*   **Agglomerative (Bottom-up):** Each data point starts as its own cluster.  Then, the algorithm iteratively merges the closest pairs of clusters until only one cluster remains, representing the entire dataset. This is the most common type.\n",
      "*   **Divisive (Top-down):**  The entire dataset starts as one cluster.  Then, the algorithm recursively splits the cluster into smaller and smaller clusters until each data point is in its own cluster.  This method is computationally more expensive than agglomerative.\n",
      "\n",
      "We will primarily focus on agglomerative clustering in this lesson.\n",
      "\n",
      "**2. Agglomerative Clustering in Detail:**\n",
      "\n",
      "The core of agglomerative clustering lies in determining the 'distance' between clusters. This distance is defined by the *linkage criterion*. Common linkage methods include:\n",
      "\n",
      "*   **Single Linkage (Nearest Point):** The distance between two clusters is the shortest distance between any two points in the clusters.  This can lead to \"chaining,\" where clusters grow long and snake-like.  Sensitive to outliers.\n",
      "\n",
      "*   **Complete Linkage (Furthest Point):** The distance between two clusters is the *longest* distance between any two points in the clusters.  Less prone to chaining, tends to produce more compact clusters.\n",
      "\n",
      "*   **Average Linkage (UPGMA):** The distance between two clusters is the *average* distance between all pairs of points in the two clusters.  A good compromise between single and complete linkage.\n",
      "\n",
      "*   **Ward Linkage:**  Minimizes the variance within each cluster.  This method looks at all possible merges of clusters and chooses the merge that minimizes the increase in the within-cluster variance.  Tends to produce more spherical clusters of similar sizes. Ward linkage should only be used with Euclidean distance.\n",
      "\n",
      "**Algorithm:**\n",
      "\n",
      "1.  Start with each data point as its own cluster.\n",
      "2.  Compute the distance matrix between all pairs of clusters.\n",
      "3.  Find the two closest clusters according to the chosen linkage criterion.\n",
      "4.  Merge these two clusters.\n",
      "5.  Update the distance matrix to reflect the new cluster.\n",
      "6.  Repeat steps 3-5 until only one cluster remains.\n",
      "\n",
      "**3. Dendrograms:**\n",
      "\n",
      "A dendrogram is a tree-like diagram that visually represents the hierarchy of clusters created by hierarchical clustering.\n",
      "\n",
      "*   The vertical axis represents the distance between clusters. The lower the join, the more similar the clusters.\n",
      "*   The horizontal axis represents the data points.\n",
      "*   By drawing a horizontal line at a certain distance threshold, you can determine the clusters that exist at that level of similarity.  The number of branches that the line intersects represents the number of clusters.\n",
      "\n",
      "**4. Choosing the Number of Clusters:**\n",
      "\n",
      "Unlike k-means, hierarchical clustering doesn't *require* you to predefine the number of clusters. However, you often want to extract a specific number of clusters from the hierarchy.  This is done by \"cutting\" the dendrogram at a certain level.\n",
      "\n",
      "*   **Visual Inspection of the Dendrogram:** Look for significant jumps in the distance between merges. A large jump suggests that merging those clusters resulted in a much less similar cluster, indicating a potential \"natural\" cluster boundary.\n",
      "\n",
      "*   **Domain Knowledge:** Your understanding of the data might suggest a reasonable number of clusters.\n",
      "\n",
      "*   **External Validation Metrics (after extracting clusters):**  After cutting the dendrogram, you can use metrics like the Silhouette score (from k-means) to evaluate the quality of the resulting clusters. A higher Silhouette score indicates better-defined clusters.  While Silhouette Score is more commonly used for K-Means, you can apply it to the results of hierarchical clustering once you've defined the number of clusters by cutting the dendrogram.\n",
      "\n",
      "**5. Advantages and Disadvantages:**\n",
      "\n",
      "**Advantages:**\n",
      "\n",
      "*   Doesn't require specifying the number of clusters beforehand.\n",
      "*   Provides a hierarchical representation of the data.\n",
      "*   Dendrograms offer a visual representation of cluster relationships.\n",
      "*   Can reveal underlying structure in the data that k-means might miss.\n",
      "\n",
      "**Disadvantages:**\n",
      "\n",
      "*   Can be computationally expensive, especially for large datasets (agglomerative clustering is typically O(n^3) in time complexity).\n",
      "*   Sensitive to noise and outliers, especially with single linkage.\n",
      "*   Can be difficult to interpret dendrograms for very large datasets.\n",
      "*   Once a merge is performed, it cannot be undone.\n",
      "\n",
      "**Examples:**\n",
      "\n",
      "**Example 1: Basic Agglomerative Clustering**\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from sklearn.cluster import AgglomerativeClustering\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Sample Data\n",
      "X = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])\n",
      "\n",
      "# Agglomerative Clustering with 2 clusters and Ward linkage\n",
      "clustering = AgglomerativeClustering(n_clusters=2, linkage='ward')\n",
      "clustering.fit(X)\n",
      "\n",
      "# Print cluster labels\n",
      "print(clustering.labels_)\n",
      "\n",
      "# Plot the clusters\n",
      "plt.scatter(X[:, 0], X[:, 1], c=clustering.labels_)\n",
      "plt.title('Agglomerative Clustering with Ward Linkage')\n",
      "plt.xlabel('Feature 1')\n",
      "plt.ylabel('Feature 2')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1.  We create sample data `X` with 6 data points.\n",
      "2.  We initialize `AgglomerativeClustering` with `n_clusters=2` and `linkage='ward'`.  Ward linkage aims to minimize the variance within each cluster.\n",
      "3.  `clustering.fit(X)` performs the clustering.\n",
      "4.  `clustering.labels_` provides the cluster assignments for each data point (0 or 1 in this case).\n",
      "5.  The plot visualizes the clusters, with different colors representing different clusters.\n",
      "\n",
      "**Example 2: Using Different Linkage Methods**\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from sklearn.cluster import AgglomerativeClustering\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Sample Data (same as above)\n",
      "X = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])\n",
      "\n",
      "# Linkage Methods to Test\n",
      "linkage_methods = ['single', 'complete', 'average', 'ward']\n",
      "\n",
      "# Iterate through each linkage method\n",
      "for linkage in linkage_methods:\n",
      "    clustering = AgglomerativeClustering(n_clusters=2, linkage=linkage)\n",
      "    clustering.fit(X)\n",
      "\n",
      "    # Plot the clusters\n",
      "    plt.figure() # create a new figure for each plot\n",
      "    plt.scatter(X[:, 0], X[:, 1], c=clustering.labels_)\n",
      "    plt.title(f'Agglomerative Clustering with {linkage} Linkage')\n",
      "    plt.xlabel('Feature 1')\n",
      "    plt.ylabel('Feature 2')\n",
      "    plt.show()\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "This example demonstrates how different linkage methods can produce different clustering results. We iterate through 'single', 'complete', 'average', and 'ward' linkage, applying agglomerative clustering with each.  The plots show how the cluster assignments change based on the linkage criterion.  Pay attention to how single linkage might create chain-like clusters and how complete linkage creates tighter clusters.  Ward linkage, aiming to minimize variance within clusters, will often result in clusters of roughly equal size (when data permits).\n",
      "\n",
      "**Example 3: Visualizing the Dendrogram**\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.cluster.hierarchy import dendrogram, linkage\n",
      "from sklearn.datasets import make_blobs\n",
      "\n",
      "# Generate sample data\n",
      "X, y = make_blobs(n_samples=50, centers=3, random_state=42)\n",
      "\n",
      "# Perform hierarchical clustering\n",
      "linked = linkage(X, 'ward') # Ward linkage is common for dendrograms\n",
      "\n",
      "# Plot the dendrogram\n",
      "plt.figure(figsize=(10, 7))\n",
      "dendrogram(linked,\n",
      "            orientation='top',\n",
      "            labels=None, # You can add labels if you have them\n",
      "            distance_sort='descending',\n",
      "            show_leaf_counts=True)\n",
      "plt.title(\"Dendrogram\")\n",
      "plt.xlabel(\"Data Points\")\n",
      "plt.ylabel(\"Distance\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1.  We use `make_blobs` to generate sample data with 50 points and 3 centers, providing a more structured dataset for clustering.  This creates data that is naturally separable.\n",
      "2.  `linkage(X, 'ward')` from `scipy.cluster.hierarchy` performs the hierarchical clustering.  Note that this `linkage` function is different from the scikit-learn class; it directly computes the linkage matrix needed for the dendrogram.\n",
      "3.  `dendrogram(linked)` plots the dendrogram.  The `orientation='top'` argument makes the dendrogram grow from top to bottom.  The 'distance' shown on the y-axis of a dendrogram represents the distances at which clusters were merged.  The height of each U-shaped join in the dendrogram represents the distance between the two clusters that were merged at that point.\n",
      "\n",
      "**Exercises:**\n",
      "\n",
      "**Exercise 1: Implement Agglomerative Clustering with Different Linkages**\n",
      "\n",
      "1.  Load the `iris` dataset from `sklearn.datasets`.\n",
      "2.  Apply `AgglomerativeClustering` with `n_clusters=3` using 'single', 'complete', and 'average' linkage methods.\n",
      "3.  Print the cluster labels for each linkage method.\n",
      "4.  Visualize the clusters for each linkage method using a scatter plot (using the first two features of the iris dataset).  Color the points according to their cluster assignment.\n",
      "\n",
      "**Exercise 2: Dendrogram Visualization and Cluster Selection**\n",
      "\n",
      "1.  Generate sample data using `make_blobs` (e.g., 50 samples, 4 centers).\n",
      "2.  Perform hierarchical clustering using `linkage` with 'ward' linkage.\n",
      "3.  Plot the dendrogram.\n",
      "4.  Based on the dendrogram, visually estimate the optimal number of clusters.\n",
      "5.  Apply `AgglomerativeClustering` with the estimated number of clusters (from step 4) and 'ward' linkage.\n",
      "6.  Print the cluster labels.\n",
      "\n",
      "**Exercise 3: Comparing Hierarchical Clustering and K-Means**\n",
      "\n",
      "1.  Load the `make_moons` dataset from `sklearn.datasets` (set `noise=0.05`). This dataset creates two interleaving half-circle shapes.\n",
      "2.  Apply both `KMeans` (with `n_clusters=2`) and `AgglomerativeClustering` (with `n_clusters=2` and 'ward' linkage) to the data.\n",
      "3.  Visualize the clustering results for both algorithms using scatter plots.\n",
      "4.  Compare the results. Which algorithm performs better on this dataset, and why?  (Hint: Consider the shapes of the clusters that each algorithm tends to produce.)\n",
      "\n",
      "**Assessments:**\n",
      "\n",
      "**Question 1:**\n",
      "\n",
      "Which of the following is NOT a linkage method in agglomerative clustering?\n",
      "\n",
      "a) Single Linkage\n",
      "b) Complete Linkage\n",
      "c) Average Linkage\n",
      "d) K-Means Linkage\n",
      "\n",
      "**Correct Answer:** d) K-Means Linkage\n",
      "\n",
      "**Question 2:**\n",
      "\n",
      "What does a dendrogram visually represent?\n",
      "\n",
      "a) The Silhouette score of different clustering solutions.\n",
      "b) The hierarchy of clusters formed by hierarchical clustering.\n",
      "c) The optimal number of clusters in a dataset.\n",
      "d) The decision boundaries of a classification model.\n",
      "\n",
      "**Correct Answer:** b) The hierarchy of clusters formed by hierarchical clustering.\n",
      "\n",
      "**Question 3:**\n",
      "\n",
      "Which linkage method is most prone to the \"chaining\" effect?\n",
      "\n",
      "a) Ward Linkage\n",
      "b) Complete Linkage\n",
      "c) Average Linkage\n",
      "d) Single Linkage\n",
      "\n",
      "**Correct Answer:** d) Single Linkage\n",
      "\n",
      "**Question 4:**\n",
      "\n",
      "True or False: Hierarchical clustering requires you to specify the number of clusters beforehand.\n",
      "\n",
      "**Correct Answer:** False\n",
      "\n",
      "**Question 5:**\n",
      "\n",
      "Which linkage method minimizes the variance within each cluster?\n",
      "\n",
      "a) Single Linkage\n",
      "b) Complete Linkage\n",
      "c) Average Linkage\n",
      "d) Ward Linkage\n",
      "\n",
      "**Correct Answer:** d) Ward Linkage\n",
      "\n",
      "**Additional Resources:**\n",
      "\n",
      "- https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
      "- https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html\n",
      "- https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/\n",
      "- https://towardsdatascience.com/understanding-the-concept-of-hierarchical-clustering-technique-c6e068683410\n",
      "- https://realpython.com/hierarchical-clustering-python/\n",
      "- https://www.datacamp.com/tutorial/hierarchical-clustering-with-python\n",
      "\n",
      "\n",
      "\n",
      "=== UNIT 5: Dimensionality Reduction ===\n",
      "\n",
      "Here's a comprehensive lesson plan for 'PCA', formatted as plain text and separated into the requested sections.\n",
      "\n",
      "**PCA Lesson Plan**\n",
      "\n",
      "**Learning Objectives:**\n",
      "\n",
      "*   Understand the principles of Principal Component Analysis (PCA).\n",
      "*   Implement PCA using Python.\n",
      "*   Evaluate the impact of PCA on model performance.\n",
      "\n",
      "**Readings:**\n",
      "\n",
      "**1. Introduction to Principal Component Analysis (PCA):**\n",
      "\n",
      "Principal Component Analysis (PCA) is a powerful dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional representation while preserving the most important information. It identifies the principal components, which are orthogonal (uncorrelated) directions that capture the maximum variance in the data.  Essentially, PCA rotates and projects the data onto a new set of axes ordered by the amount of variance they explain.\n",
      "\n",
      "*   **Why Use PCA?**  Data often contains redundant or irrelevant features. High dimensionality can lead to increased computational cost, overfitting in machine learning models (the \"curse of dimensionality\"), and difficulty in visualizing data. PCA helps address these issues by reducing the number of features while retaining crucial information.\n",
      "\n",
      "*   **Key Concepts:**\n",
      "    *   **Variance:** A measure of how spread out the data is along a particular direction. PCA aims to find directions (principal components) with high variance.\n",
      "    *   **Covariance:** Measures how two variables change together. A positive covariance indicates a positive correlation, a negative covariance indicates a negative correlation, and a covariance of zero indicates no linear relationship.  PCA uses the covariance matrix to understand the relationships between features.\n",
      "    *   **Eigenvectors and Eigenvalues:** Eigenvectors are special vectors that, when multiplied by a matrix, only change in scale.  The scaling factor is the eigenvalue. In PCA, eigenvectors represent the principal components (directions), and eigenvalues represent the amount of variance explained by each corresponding principal component.  Higher eigenvalues indicate more important principal components.\n",
      "    *   **Orthogonality:** Principal components are orthogonal (perpendicular) to each other.  This means they are uncorrelated and capture different aspects of the data's variance.\n",
      "\n",
      "**2. Steps of PCA:**\n",
      "\n",
      "1.  **Data Standardization:**  Scale the data to have zero mean and unit variance. This is crucial because PCA is sensitive to the scale of the features.  Use methods like StandardScaler in scikit-learn.  Formula: `z = (x - u) / s`, where `z` is the standardized value, `x` is the original value, `u` is the mean, and `s` is the standard deviation.\n",
      "\n",
      "2.  **Covariance Matrix Calculation:** Calculate the covariance matrix of the standardized data. The covariance matrix summarizes the relationships between all pairs of features.\n",
      "\n",
      "3.  **Eigenvalue Decomposition:** Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
      "\n",
      "4.  **Select Principal Components:** Sort the eigenvalues in descending order. Choose the top *k* eigenvectors corresponding to the *k* largest eigenvalues, where *k* is the desired number of principal components (k < number of original features).  The number of components chosen depends on the amount of variance you want to retain, which is typically expressed as a percentage (e.g., retaining 95% of the variance).\n",
      "\n",
      "5.  **Feature Transformation:**  Project the original data onto the selected principal components.  This is done by multiplying the standardized data by the matrix of eigenvectors (the principal components). This projection creates the new, lower-dimensional dataset.\n",
      "\n",
      "**3. Choosing the Number of Principal Components:**\n",
      "\n",
      "Determining the optimal number of principal components is crucial for effective dimensionality reduction. Several methods can be used:\n",
      "\n",
      "*   **Explained Variance Ratio:** The explained variance ratio of a principal component is the proportion of the total variance in the data explained by that component. It's calculated as `eigenvalue / sum(eigenvalues)`.  Plot the cumulative explained variance ratio against the number of components.  Look for an \"elbow\" in the plot, where adding more components provides diminishing returns in terms of explained variance. A common target is to retain 95% or 99% of the variance.\n",
      "\n",
      "*   **Scree Plot:**  A scree plot shows the eigenvalues plotted against the component number.  Similar to the explained variance ratio plot, look for an elbow in the scree plot. The components to the left of the elbow are generally considered more important.\n",
      "\n",
      "*   **Cross-Validation:** If PCA is used as a preprocessing step for a machine learning model, you can use cross-validation to evaluate the performance of the model with different numbers of principal components. Choose the number of components that yields the best cross-validation score.\n",
      "\n",
      "**4. PCA in Scikit-Learn:**\n",
      "\n",
      "Scikit-learn provides a convenient `PCA` class for implementing PCA.\n",
      "\n",
      "*   `PCA(n_components=None)`: Creates a PCA object. `n_components` specifies the number of components to keep. If `n_components` is not specified or set to `None`, all components are kept.  You can also specify `n_components` as a float between 0 and 1 to indicate the desired variance ratio to retain.\n",
      "\n",
      "*   `fit(X)`: Fits the PCA model to the data `X`.  `X` should be a NumPy array or Pandas DataFrame.\n",
      "\n",
      "*   `transform(X)`: Applies the dimensionality reduction to `X` and returns the transformed data.\n",
      "\n",
      "*   `fit_transform(X)`: A shortcut for calling `fit(X)` followed by `transform(X)`.\n",
      "\n",
      "*   `explained_variance_ratio_`: An attribute of the fitted PCA object that returns the explained variance ratio for each component.\n",
      "\n",
      "*   `components_`: An attribute of the fitted PCA object that returns the principal components (eigenvectors).\n",
      "\n",
      "**5. Limitations of PCA:**\n",
      "\n",
      "*   **Linearity:** PCA is a linear technique. It may not be effective for data with highly non-linear relationships.  Consider non-linear dimensionality reduction techniques like t-SNE or UMAP for such data.\n",
      "\n",
      "*   **Data Scaling:** PCA is sensitive to the scale of the features.  Standardization is crucial before applying PCA.\n",
      "\n",
      "*   **Interpretability:** While PCA reduces dimensionality, the resulting principal components may not always be easily interpretable in terms of the original features.\n",
      "\n",
      "**Examples:**\n",
      "\n",
      "**1. Basic PCA Implementation:**\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Sample Data\n",
      "data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
      "\n",
      "# Standardize the data\n",
      "scaler = StandardScaler()\n",
      "scaled_data = scaler.fit_transform(data)\n",
      "\n",
      "# Apply PCA\n",
      "pca = PCA(n_components=2)  # Reduce to 2 components\n",
      "pca.fit(scaled_data)\n",
      "\n",
      "# Transform the data\n",
      "transformed_data = pca.transform(scaled_data)\n",
      "\n",
      "print(\"Original Data:\\n\", data)\n",
      "print(\"\\nScaled Data:\\n\", scaled_data)\n",
      "print(\"\\nTransformed Data:\\n\", transformed_data)\n",
      "print(\"\\nExplained Variance Ratio:\", pca.explained_variance_ratio_)\n",
      "```\n",
      "\n",
      "*Explanation:* This example demonstrates a basic PCA implementation. The data is first standardized using `StandardScaler`. A PCA object is created to reduce the data to 2 components. The `fit` method learns the principal components from the scaled data, and the `transform` method applies the dimensionality reduction. The `explained_variance_ratio_` attribute shows how much variance each principal component explains.\n",
      "\n",
      "**2. Choosing the Number of Components using Explained Variance Ratio:**\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "# Load the Iris dataset\n",
      "iris = load_iris()\n",
      "X = iris.data\n",
      "\n",
      "# Standardize the data\n",
      "scaler = StandardScaler()\n",
      "scaled_data = scaler.fit_transform(X)\n",
      "\n",
      "# Apply PCA with all components\n",
      "pca = PCA()\n",
      "pca.fit(scaled_data)\n",
      "\n",
      "# Calculate cumulative explained variance ratio\n",
      "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
      "\n",
      "# Plot the cumulative explained variance ratio\n",
      "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance)\n",
      "plt.xlabel(\"Number of Components\")\n",
      "plt.ylabel(\"Cumulative Explained Variance\")\n",
      "plt.title(\"Cumulative Explained Variance vs. Number of Components\")\n",
      "plt.grid(True)\n",
      "plt.show()\n",
      "\n",
      "# Determine the number of components to retain 95% variance\n",
      "n_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
      "print(f\"Number of components to retain 95% variance: {n_components}\")\n",
      "```\n",
      "\n",
      "*Explanation:* This example uses the Iris dataset to demonstrate how to choose the number of components based on the explained variance ratio. The cumulative explained variance ratio is calculated and plotted. The code identifies the number of components needed to retain at least 95% of the variance. The plot helps visualize the elbow method.\n",
      "\n",
      "**3. PCA for Model Improvement (Logistic Regression):**\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.datasets import load_digits\n",
      "\n",
      "# Load the digits dataset\n",
      "digits = load_digits()\n",
      "X, y = digits.data, digits.target\n",
      "\n",
      "# Split into training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
      "\n",
      "# Standardize the data\n",
      "scaler = StandardScaler()\n",
      "X_train_scaled = scaler.fit_transform(X_train)\n",
      "X_test_scaled = scaler.transform(X_test)\n",
      "\n",
      "# Baseline Logistic Regression (without PCA)\n",
      "lr_baseline = LogisticRegression(solver='liblinear', multi_class='ovr', random_state=42)\n",
      "lr_baseline.fit(X_train_scaled, y_train)\n",
      "y_pred_baseline = lr_baseline.predict(X_test_scaled)\n",
      "accuracy_baseline = accuracy_score(y_test, y_pred_baseline)\n",
      "print(f\"Baseline Accuracy (without PCA): {accuracy_baseline:.4f}\")\n",
      "\n",
      "# PCA with 30 components\n",
      "pca = PCA(n_components=30)\n",
      "X_train_pca = pca.fit_transform(X_train_scaled)\n",
      "X_test_pca = pca.transform(X_test_scaled)\n",
      "\n",
      "# Logistic Regression with PCA\n",
      "lr_pca = LogisticRegression(solver='liblinear', multi_class='ovr', random_state=42)\n",
      "lr_pca.fit(X_train_pca, y_train)\n",
      "y_pred_pca = lr_pca.predict(X_test_pca)\n",
      "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
      "print(f\"Accuracy with PCA (30 components): {accuracy_pca:.4f}\")\n",
      "```\n",
      "\n",
      "*Explanation:* This example demonstrates how PCA can be used to improve the performance of a Logistic Regression model on the digits dataset. The code compares the accuracy of the model with and without PCA.  PCA reduces the number of features to 30, potentially improving generalization and reducing overfitting.\n",
      "\n",
      "**Exercises:**\n",
      "\n",
      "1.  **Implement PCA from Scratch (Optional):** Write a Python function that implements PCA without using scikit-learn. The function should take a data matrix and the desired number of components as input and return the transformed data. (This exercise is more challenging).\n",
      "\n",
      "2.  **PCA on a Real Dataset:** Download a real-world dataset (e.g., from Kaggle or the UCI Machine Learning Repository). Apply PCA to the dataset and visualize the first two principal components using a scatter plot.  Choose an appropriate number of components to retain 90% of the variance.\n",
      "\n",
      "3.  **Impact of PCA on a Classification Model:** Use the Breast Cancer dataset from scikit-learn. Train a Support Vector Machine (SVM) classifier with and without PCA. Compare the performance (accuracy, precision, recall, F1-score) of the two models. Experiment with different numbers of principal components.\n",
      "\n",
      "4.  **PCA and Visualization:** Load the Fashion MNIST dataset.  Apply PCA to reduce the data to two dimensions.  Create a scatter plot of the reduced data, color-coding the points by their class labels.  Analyze the plot: Can you observe any separation between the classes?\n",
      "\n",
      "**Assessments:**\n",
      "\n",
      "Instructions: Select the best answer for each question.\n",
      "\n",
      "1.  **What is the primary goal of Principal Component Analysis (PCA)?**\n",
      "    (a) Feature Selection\n",
      "    (b) Dimensionality Reduction\n",
      "    (c) Feature Engineering\n",
      "    (d) Data Normalization\n",
      "    *Answer: (b)*\n",
      "\n",
      "2.  **Which of the following steps is typically performed *before* applying PCA?**\n",
      "    (a) Eigenvalue Decomposition\n",
      "    (b) Data Standardization\n",
      "    (c) Feature Transformation\n",
      "    (d) Covariance Matrix Calculation\n",
      "    *Answer: (b)*\n",
      "\n",
      "3.  **What do eigenvectors represent in PCA?**\n",
      "    (a) The original features\n",
      "    (b) The amount of variance explained by each feature\n",
      "    (c) The principal components (directions of maximum variance)\n",
      "    (d) The standardized data\n",
      "    *Answer: (c)*\n",
      "\n",
      "4.  **What do eigenvalues represent in PCA?**\n",
      "    (a) The principal components\n",
      "    (b) The data points\n",
      "    (c) The amount of variance explained by each principal component\n",
      "    (d) The original features\n",
      "    *Answer: (c)*\n",
      "\n",
      "5.  **The explained variance ratio of a principal component is calculated as:**\n",
      "    (a) `sum(eigenvalues) / eigenvalue`\n",
      "    (b) `eigenvalue * sum(eigenvalues)`\n",
      "    (c) `eigenvalue / sum(eigenvalues)`\n",
      "    (d) `1 / eigenvalue`\n",
      "    *Answer: (c)*\n",
      "\n",
      "6.  **Which of the following is a limitation of PCA?**\n",
      "    (a) It is not suitable for high-dimensional data.\n",
      "    (b) It is sensitive to the scale of the features.\n",
      "    (c) It always improves the accuracy of machine learning models.\n",
      "    (d) It cannot handle missing data.\n",
      "    *Answer: (b)*\n",
      "\n",
      "7.  **In scikit-learn, which method is used to apply PCA to a dataset `X` after fitting the model?**\n",
      "    (a) `fit(X)`\n",
      "    (b) `predict(X)`\n",
      "    (c) `transform(X)`\n",
      "    (d) `fit_transform(X)`\n",
      "    *Answer: (c)*\n",
      "\n",
      "8.  **A scree plot is used to:**\n",
      "    (a) Visualize the original data in 2D.\n",
      "    (b) Determine the optimal number of principal components.\n",
      "    (c) Calculate the covariance matrix.\n",
      "    (d) Standardize the data.\n",
      "    *Answer: (b)*\n",
      "\n",
      "9.  **If you want to retain 90% of the variance in your data using PCA, you should choose the number of components such that the cumulative explained variance ratio is:**\n",
      "    (a) Equal to 0.1\n",
      "    (b) Less than 0.9\n",
      "    (c) Greater than or equal to 0.9\n",
      "    (d) Always equal to 1\n",
      "    *Answer: (c)*\n",
      "\n",
      "10. **When should you consider using non-linear dimensionality reduction techniques instead of PCA?**\n",
      "    (a) When the data has high dimensionality.\n",
      "    (b) When the data has linear relationships between features.\n",
      "    (c) When the data has non-linear relationships between features.\n",
      "    (d) When you want to reduce the computational cost of machine learning.\n",
      "    *Answer: (c)*\n",
      "\n",
      "**Additional Resources:**\n",
      "\n",
      "- https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
      "- https://towardsdatascience.com/principal-component-analysis-pca-explained-visually-with-zero-math-1c63df693e72\n",
      "- https://www.datacamp.com/tutorial/principal-component-analysis-in-python\n",
      "- https://builtin.com/data-science/pca-principal-component-analysis\n",
      "- https://www.geeksforgeeks.org/principal-component-analysis-pca/\n",
      "- https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-with-examples-in-python-e0246052169c\n",
      "\n",
      "\n",
      "\n",
      "=== UNIT 5: Dimensionality Reduction ===\n",
      "\n",
      "```\n",
      "## Applications of PCA: A Comprehensive Lesson\n",
      "\n",
      "**Learning Objectives:**\n",
      "\n",
      "*   Understand the principles of Principal Component Analysis (PCA).\n",
      "*   Implement PCA using Python.\n",
      "*   Evaluate the impact of PCA on model performance.\n",
      "\n",
      "**Readings:**\n",
      "\n",
      "**Section 1: Introduction to Principal Component Analysis (PCA)**\n",
      "\n",
      "Principal Component Analysis (PCA) is a powerful dimensionality reduction technique used in various fields, including machine learning, image processing, and data visualization. Its primary goal is to transform a high-dimensional dataset into a lower-dimensional representation while preserving the most important information. PCA achieves this by identifying orthogonal axes (principal components) that capture the maximum variance in the data.\n",
      "\n",
      "**Key Concepts:**\n",
      "\n",
      "*   **Dimensionality Reduction:** Reducing the number of variables (features) in a dataset.\n",
      "*   **Variance:** A measure of how spread out the data is. PCA aims to find components that capture the most variance.\n",
      "*   **Principal Components:** New, uncorrelated variables that are linear combinations of the original variables. They are ordered by the amount of variance they explain. The first principal component explains the most variance, the second explains the second most, and so on.\n",
      "*   **Orthogonality:** Principal components are orthogonal, meaning they are uncorrelated and perpendicular to each other.\n",
      "*   **Eigenvalues and Eigenvectors:** Eigenvalues represent the amount of variance explained by each principal component, and eigenvectors define the direction of these components. PCA relies on eigenvalue decomposition of the covariance matrix.\n",
      "\n",
      "**The PCA Process:**\n",
      "\n",
      "1.  **Standardize the Data:** Ensure all features have a mean of 0 and a standard deviation of 1. This is crucial to prevent features with larger scales from dominating the PCA process.  Use StandardScaler from scikit-learn.\n",
      "2.  **Calculate the Covariance Matrix:** Compute the covariance matrix of the standardized data. This matrix describes the relationships between the different features.\n",
      "3.  **Eigenvalue Decomposition:** Perform eigenvalue decomposition on the covariance matrix. This yields the eigenvalues and eigenvectors.\n",
      "4.  **Sort Eigenvalues and Eigenvectors:** Sort the eigenvalues in descending order. The eigenvector corresponding to the largest eigenvalue is the first principal component, the eigenvector corresponding to the second largest eigenvalue is the second principal component, and so on.\n",
      "5.  **Select Principal Components:** Choose the top *k* eigenvectors (principal components) that capture a significant portion of the variance. The number *k* is determined by the desired level of dimensionality reduction. You can use explained variance ratio to guide your decision of how many components to keep.\n",
      "6.  **Project the Data:** Project the original data onto the selected principal components. This creates the lower-dimensional representation of the data.\n",
      "\n",
      "**Benefits of PCA:**\n",
      "\n",
      "*   **Dimensionality Reduction:** Simplifies data and reduces computational cost.\n",
      "*   **Noise Reduction:** By discarding components with low variance, PCA can reduce noise in the data.\n",
      "*   **Data Visualization:**  Reduces data to 2 or 3 dimensions for easier plotting and visualization.\n",
      "*   **Feature Extraction:** Creates new, uncorrelated features that can be used in machine learning models.\n",
      "\n",
      "**Limitations of PCA:**\n",
      "\n",
      "*   **Linearity:** PCA is a linear technique and may not be effective for non-linear data.\n",
      "*   **Interpretability:** Principal components can be difficult to interpret, as they are linear combinations of the original features.\n",
      "*   **Data Scaling:** PCA is sensitive to data scaling, so standardization is essential.\n",
      "\n",
      "**Section 2: PCA Implementation in Python**\n",
      "\n",
      "The scikit-learn library provides a convenient implementation of PCA.  We'll use `sklearn.decomposition.PCA`.\n",
      "\n",
      "**Section 3: Evaluating the Impact of PCA on Model Performance**\n",
      "\n",
      "PCA can improve model performance in several ways:\n",
      "\n",
      "*   **Reduced Overfitting:** By reducing the number of features, PCA can help prevent overfitting, especially when the number of features is close to or greater than the number of samples.\n",
      "*   **Improved Computational Efficiency:** Reduced dimensionality leads to faster training and prediction times.\n",
      "*   **Improved Accuracy:** In some cases, PCA can improve accuracy by removing noise and irrelevant features.\n",
      "\n",
      "However, PCA can also negatively impact model performance if too many principal components are discarded, leading to a loss of important information.\n",
      "\n",
      "**Methods for Evaluating PCA's Impact:**\n",
      "\n",
      "*   **Train and Evaluate Models:** Train and evaluate machine learning models (e.g., logistic regression, support vector machines, random forests) with and without PCA. Compare the performance metrics (e.g., accuracy, precision, recall, F1-score) to assess the impact of PCA.\n",
      "*   **Explained Variance Ratio:**  Analyze the explained variance ratio for each principal component. This helps determine the optimal number of components to retain.  A cumulative explained variance plot can be a useful visualization.\n",
      "*   **Cross-Validation:** Use cross-validation to obtain more robust estimates of model performance with and without PCA.\n",
      "\n",
      "**Examples:**\n",
      "\n",
      "**Example 1: PCA for Data Visualization**\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.datasets import load_iris\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Load the Iris dataset\n",
      "iris = load_iris()\n",
      "X = iris.data\n",
      "y = iris.target\n",
      "\n",
      "# Standardize the data\n",
      "scaler = StandardScaler()\n",
      "X_scaled = scaler.fit_transform(X)\n",
      "\n",
      "# Apply PCA with 2 components\n",
      "pca = PCA(n_components=2)\n",
      "X_pca = pca.fit_transform(X_scaled)\n",
      "\n",
      "# Visualize the data\n",
      "plt.figure(figsize=(8, 6))\n",
      "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
      "plt.xlabel('Principal Component 1')\n",
      "plt.ylabel('Principal Component 2')\n",
      "plt.title('PCA of Iris Dataset')\n",
      "plt.colorbar(label='Species')\n",
      "plt.show()\n",
      "\n",
      "# Print explained variance ratio\n",
      "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1.  We load the Iris dataset, which has 4 features.\n",
      "2.  We standardize the data using `StandardScaler`.\n",
      "3.  We apply PCA with `n_components=2` to reduce the dimensionality to 2.\n",
      "4.  We plot the data in the reduced 2D space, with each point colored according to its species.\n",
      "5.  We print the explained variance ratio for each principal component. This tells us how much variance is explained by each component.\n",
      "\n",
      "**Example 2: PCA for Improving Model Performance**\n",
      "\n",
      "```python\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.datasets import load_breast_cancer\n",
      "\n",
      "# Load the Breast Cancer dataset\n",
      "cancer = load_breast_cancer()\n",
      "X = cancer.data\n",
      "y = cancer.target\n",
      "\n",
      "# Split the data into training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
      "\n",
      "# Standardize the data\n",
      "scaler = StandardScaler()\n",
      "X_train_scaled = scaler.fit_transform(X_train)\n",
      "X_test_scaled = scaler.transform(X_test)\n",
      "\n",
      "# Train a Logistic Regression model without PCA\n",
      "model_no_pca = LogisticRegression(solver='liblinear', random_state=42)\n",
      "model_no_pca.fit(X_train_scaled, y_train)\n",
      "y_pred_no_pca = model_no_pca.predict(X_test_scaled)\n",
      "accuracy_no_pca = accuracy_score(y_test, y_pred_no_pca)\n",
      "print(\"Accuracy without PCA:\", accuracy_no_pca)\n",
      "\n",
      "# Apply PCA with 10 components\n",
      "pca = PCA(n_components=10)\n",
      "X_train_pca = pca.fit_transform(X_train_scaled)\n",
      "X_test_pca = pca.transform(X_test_scaled)\n",
      "\n",
      "# Train a Logistic Regression model with PCA\n",
      "model_pca = LogisticRegression(solver='liblinear', random_state=42)\n",
      "model_pca.fit(X_train_pca, y_train)\n",
      "y_pred_pca = model_pca.predict(X_test_pca)\n",
      "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
      "print(\"Accuracy with PCA:\", accuracy_pca)\n",
      "\n",
      "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
      "\n",
      "# Plot cumulative explained variance\n",
      "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
      "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance)\n",
      "plt.xlabel('Number of Components')\n",
      "plt.ylabel('Cumulative Explained Variance')\n",
      "plt.title('Cumulative Explained Variance vs. Number of Components')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1.  We load the Breast Cancer dataset, which has 30 features.\n",
      "2.  We split the data into training and testing sets.\n",
      "3.  We standardize the data using `StandardScaler`.\n",
      "4.  We train a Logistic Regression model *without* PCA and evaluate its accuracy.\n",
      "5.  We apply PCA with `n_components=10` to reduce the dimensionality to 10.\n",
      "6.  We train a Logistic Regression model *with* PCA and evaluate its accuracy.\n",
      "7.  We compare the accuracy of the two models. In this case, using PCA might improve performance slightly, reduce overfitting, and reduce computational cost.\n",
      "8.  We plot the cumulative explained variance to help visualize how much variance is captured by each component.\n",
      "\n",
      "**Exercises:**\n",
      "\n",
      "**Exercise 1: Implementing PCA from Scratch (Optional - Advanced)**\n",
      "\n",
      "1.  Create a function `pca(X, n_components)` that implements PCA from scratch. The function should take a data matrix `X` and the desired number of components `n_components` as input and return the projected data matrix.\n",
      "2.  Test your function on the Iris dataset and compare the results with the scikit-learn implementation.\n",
      "\n",
      "**Exercise 2: PCA on MNIST Dataset**\n",
      "\n",
      "1.  Load the MNIST dataset (handwritten digits).  You can use `sklearn.datasets.load_digits()`.\n",
      "2.  Apply PCA to reduce the dimensionality to 50 components.\n",
      "3.  Visualize the first 10 principal components as images.  You'll need to reshape the components back into image form (e.g., 8x8).\n",
      "4.  Train a simple classifier (e.g., Logistic Regression) on the original MNIST data and on the PCA-reduced data. Compare the performance and training time.\n",
      "\n",
      "**Exercise 3: Finding Optimal Number of Components**\n",
      "\n",
      "1.  Load the Breast Cancer dataset.\n",
      "2.  Apply PCA and calculate the explained variance ratio for each component.\n",
      "3.  Create a plot of the cumulative explained variance ratio versus the number of components.\n",
      "4.  Determine the number of components required to capture at least 95% of the variance in the data.\n",
      "5.  Train a Logistic Regression model with that number of components and compare the accuracy to the model trained on the original data.\n",
      "\n",
      "**Assessments:**\n",
      "\n",
      "**Question 1:**\n",
      "\n",
      "Which of the following is the primary goal of Principal Component Analysis (PCA)?\n",
      "\n",
      "a) To increase the dimensionality of a dataset.\n",
      "b) To identify the most correlated features in a dataset.\n",
      "c) To reduce the dimensionality of a dataset while preserving the most important information.\n",
      "d) To perform feature selection by choosing the best original features.\n",
      "\n",
      "**Correct Answer:** c\n",
      "\n",
      "**Question 2:**\n",
      "\n",
      "What is the purpose of standardizing the data before applying PCA?\n",
      "\n",
      "a) To make the data easier to interpret.\n",
      "b) To prevent features with larger scales from dominating the PCA process.\n",
      "c) To increase the variance in the data.\n",
      "d) It's not necessary to standardize data before applying PCA.\n",
      "\n",
      "**Correct Answer:** b\n",
      "\n",
      "**Question 3:**\n",
      "\n",
      "Eigenvalues in PCA represent:\n",
      "\n",
      "a) The direction of the principal components.\n",
      "b) The amount of variance explained by each principal component.\n",
      "c) The correlation between the original features.\n",
      "d) The mean of the original features.\n",
      "\n",
      "**Correct Answer:** b\n",
      "\n",
      "**Question 4:**\n",
      "\n",
      "True or False: Principal Components are always correlated with each other.\n",
      "\n",
      "a) True\n",
      "b) False\n",
      "\n",
      "**Correct Answer:** b\n",
      "\n",
      "**Question 5:**\n",
      "\n",
      "How can PCA help improve model performance? Select all that apply:\n",
      "\n",
      "a) By increasing the number of features.\n",
      "b) By reducing overfitting.\n",
      "c) By improving computational efficiency.\n",
      "d) By always improving accuracy regardless of the dataset.\n",
      "\n",
      "**Correct Answer:** b, c\n",
      "\n",
      "**Question 6:**\n",
      "\n",
      "You have applied PCA and found that the first two principal components explain 90% of the variance in your data. You can conclude that:\n",
      "\n",
      "a) The original dataset is perfectly represented by the first two principal components.\n",
      "b) The first two principal components capture a significant portion of the variance, making them useful for dimensionality reduction.\n",
      "c) PCA was not effective because it didn't explain 100% of the variance.\n",
      "d) You should always use all principal components to avoid losing information.\n",
      "\n",
      "**Correct Answer:** b\n",
      "\n",
      "**Additional Resources:**\n",
      "\n",
      "- https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
      "- https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c\n",
      "- https://www.datacamp.com/tutorial/principal-component-analysis-in-python\n",
      "- https://builtin.com/data-science/step-step-explanation-principal-component-analysis\n",
      "- https://www.analyticsvidhya.com/blog/2016/03/one-stop-guide-principal-component-analysis/\n",
      "- https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "=== FINAL EXAM ===\n",
      "\n",
      "```\n",
      "**Final Exam - Introduction to Machine Learning**\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "*   This exam is worth 100 points.\n",
      "*   Please read each question carefully before answering.\n",
      "*   Show your work for all calculations and coding problems. Partial credit may be awarded.\n",
      "*   You have 3 hours to complete the exam.\n",
      "\n",
      "**Section 1: Multiple Choice (2 points each, 20 points total)**\n",
      "\n",
      "Choose the best answer for each question.\n",
      "\n",
      "1.  Which of the following is NOT a type of supervised learning?\n",
      "    a) Regression\n",
      "    b) Classification\n",
      "    c) Clustering\n",
      "    d) Decision Trees\n",
      "\n",
      "2.  What is the purpose of splitting a dataset into training and testing sets?\n",
      "    a) To increase the size of the dataset.\n",
      "    b) To evaluate the performance of the model on unseen data.\n",
      "    c) To train the model twice as fast.\n",
      "    d) To reduce the risk of overfitting the training data.\n",
      "\n",
      "3.  Which evaluation metric is most appropriate for a binary classification problem with imbalanced classes?\n",
      "    a) Accuracy\n",
      "    b) Precision\n",
      "    c) Recall\n",
      "    d) F1-score\n",
      "\n",
      "4.  Which of the following algorithms is most sensitive to outliers?\n",
      "    a) Decision Tree\n",
      "    b) K-Nearest Neighbors (KNN)\n",
      "    c) Support Vector Machine (SVM)\n",
      "    d) Random Forest\n",
      "\n",
      "5.  What is the purpose of regularization in machine learning?\n",
      "    a) To speed up training.\n",
      "    b) To prevent overfitting.\n",
      "    c) To increase the accuracy on the training data.\n",
      "    d) To reduce the complexity of the model.\n",
      "\n",
      "6.  Which of the following is a dimensionality reduction technique?\n",
      "    a) One-hot encoding\n",
      "    b) Feature scaling\n",
      "    c) Principal Component Analysis (PCA)\n",
      "    d) Polynomial feature expansion\n",
      "\n",
      "7.  Which of the following is an example of unsupervised learning?\n",
      "    a) Linear Regression\n",
      "    b) Logistic Regression\n",
      "    c) K-Means Clustering\n",
      "    d) Naive Bayes\n",
      "\n",
      "8. What does the term \"bias-variance tradeoff\" refer to?\n",
      "    a) The tradeoff between model complexity and data size.\n",
      "    b) The tradeoff between underfitting and overfitting.\n",
      "    c) The tradeoff between different evaluation metrics.\n",
      "    d) The tradeoff between feature engineering and model selection.\n",
      "\n",
      "9. What is cross-validation primarily used for?\n",
      "    a) To speed up the training process.\n",
      "    b) To estimate the model's performance on unseen data and reduce variance in the evaluation.\n",
      "    c) To select the optimal learning rate.\n",
      "    d) To visualize the decision boundary of a classifier.\n",
      "\n",
      "10. Which of the following techniques is most suitable for handling missing numerical data?\n",
      "    a) Removing the rows with missing values\n",
      "    b) Replacing missing values with the mean or median\n",
      "    c) Replacing missing values with zero\n",
      "    d) All of the above can be suitable depending on the situation\n",
      "\n",
      "**Section 2: Short Answer (5 points each, 30 points total)**\n",
      "\n",
      "Answer each question concisely and completely.\n",
      "\n",
      "1.  Explain the difference between precision and recall. Give an example scenario where high recall is more important than high precision.\n",
      "\n",
      "2.  Describe the concept of overfitting. What are two strategies to mitigate overfitting?\n",
      "\n",
      "3.  What is the \"curse of dimensionality\"? How can dimensionality reduction techniques help?\n",
      "\n",
      "4.  Explain the difference between batch gradient descent and stochastic gradient descent.\n",
      "\n",
      "5.  Describe the purpose of feature scaling and provide two common feature scaling techniques.\n",
      "\n",
      "6.  Explain the difference between a generative and discriminative model. Give an example of each.\n",
      "\n",
      "**Section 3: Coding Problems (50 points total)**\n",
      "\n",
      "For each problem, provide Python code that implements the requested functionality. You may use libraries such as NumPy, pandas, and scikit-learn. Clearly comment your code.\n",
      "\n",
      "1. **KNN Implementation (20 points)**\n",
      "\n",
      "   Write a Python function `knn_predict(X_train, y_train, X_test, k)` that implements the k-Nearest Neighbors algorithm for classification.\n",
      "   *   `X_train`: A NumPy array of shape (n_train_samples, n_features) representing the training data.\n",
      "   *   `y_train`: A NumPy array of shape (n_train_samples,) representing the training labels.\n",
      "   *   `X_test`: A NumPy array of shape (n_test_samples, n_features) representing the test data.\n",
      "   *   `k`: The number of neighbors to consider.\n",
      "   *   The function should return a NumPy array of shape (n_test_samples,) representing the predicted labels for the test data. Use Euclidean distance to measure the distance between points.  In case of ties, predict the label of the closest neighbour.\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "def knn_predict(X_train, y_train, X_test, k):\n",
      "    \"\"\"\n",
      "    Predicts the labels for the test data using k-Nearest Neighbors.\n",
      "\n",
      "    Args:\n",
      "        X_train: Training data.\n",
      "        y_train: Training labels.\n",
      "        X_test: Test data.\n",
      "        k: Number of neighbors.\n",
      "\n",
      "    Returns:\n",
      "        Predicted labels for the test data.\n",
      "    \"\"\"\n",
      "    y_pred = []\n",
      "    for x in X_test:\n",
      "        # Calculate distances to all training points\n",
      "        distances = np.sqrt(np.sum((X_train - x)**2, axis=1))\n",
      "\n",
      "        # Find the k nearest neighbors\n",
      "        k_nearest_indices = np.argsort(distances)[:k]\n",
      "        k_nearest_labels = y_train[k_nearest_indices]\n",
      "\n",
      "        # Predict the label based on the majority class among the k nearest neighbors\n",
      "        # In case of ties, predict the label of the closest neighbor\n",
      "        counts = np.bincount(k_nearest_labels)\n",
      "        if np.count_nonzero(counts == np.max(counts)) > 1:\n",
      "            # Tiebreaker: predict label of the closest neighbour.\n",
      "            y_pred.append(y_train[k_nearest_indices[0]])\n",
      "        else:\n",
      "            y_pred.append(np.argmax(counts))\n",
      "\n",
      "    return np.array(y_pred)\n",
      "```\n",
      "\n",
      "2. **Feature Engineering (15 points)**\n",
      "\n",
      "Given a Pandas DataFrame `df` with a column named \"date\" in the format \"YYYY-MM-DD\", create two new columns: \"month\" and \"day_of_week\".  The \"month\" column should contain the month number (1-12), and the \"day_of_week\" column should contain the day of the week as an integer (0-6, where 0 is Monday and 6 is Sunday).\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "def create_date_features(df):\n",
      "    \"\"\"\n",
      "    Creates 'month' and 'day_of_week' features from a 'date' column.\n",
      "\n",
      "    Args:\n",
      "        df: Pandas DataFrame with a 'date' column.\n",
      "\n",
      "    Returns:\n",
      "        Pandas DataFrame with the new features.\n",
      "    \"\"\"\n",
      "    df['date'] = pd.to_datetime(df['date'])  # Convert 'date' column to datetime objects\n",
      "    df['month'] = df['date'].dt.month\n",
      "    df['day_of_week'] = df['date'].dt.dayofweek\n",
      "    return df\n",
      "```\n",
      "\n",
      "3. **Model Evaluation (15 points)**\n",
      "\n",
      "Given two NumPy arrays, `y_true` (true labels) and `y_pred` (predicted labels) for a binary classification problem, calculate and print the following metrics: accuracy, precision, recall, and F1-score.  Implement the calculations directly (do not use scikit-learn's functions for this).\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "def evaluate_binary_classification(y_true, y_pred):\n",
      "    \"\"\"\n",
      "    Calculates and prints evaluation metrics for binary classification.\n",
      "\n",
      "    Args:\n",
      "        y_true: True labels (NumPy array).\n",
      "        y_pred: Predicted labels (NumPy array).\n",
      "    \"\"\"\n",
      "\n",
      "    # Calculate confusion matrix values\n",
      "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
      "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
      "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
      "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
      "\n",
      "    # Calculate metrics\n",
      "    accuracy = (tp + tn) / len(y_true) if len(y_true) > 0 else 0\n",
      "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
      "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
      "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
      "\n",
      "    print(f\"Accuracy: {accuracy:.4f}\")\n",
      "    print(f\"Precision: {precision:.4f}\")\n",
      "    print(f\"Recall: {recall:.4f}\")\n",
      "    print(f\"F1-Score: {f1_score:.4f}\")\n",
      "```\n",
      "```python\n",
      "#Example usage\n",
      "# Sample DataFrames for testing\n",
      "data = {'date': ['2023-01-15', '2023-02-28', '2023-03-10']}\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Sample numpy arrays for testing\n",
      "y_true = np.array([1, 0, 1, 1, 0])\n",
      "y_pred = np.array([1, 1, 0, 1, 0])\n",
      "\n",
      "# Run the tests here:\n",
      "create_date_features(df.copy())\n",
      "evaluate_binary_classification(y_true, y_pred)\n",
      "\n",
      "```\n",
      "```output\n",
      "Accuracy: 0.6000\n",
      "Precision: 0.6667\n",
      "Recall: 0.6667\n",
      "F1-Score: 0.6667\n",
      "```\n",
      "**End of Exam**\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "9a96ddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def test_lesson_content_json(syllabus_json_str: str) -> str:\n",
    "    # Parse input\n",
    "    all_content = \"\"\n",
    "    try:\n",
    "        syllabus_data = json.loads(syllabus_json_str)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Invalid JSON input. Error: {e}\")\n",
    "\n",
    "    # First unit and first lesson\n",
    "    first_unit = syllabus_data[\"course\"][\"units\"][0]\n",
    "    first_lesson = first_unit[\"lesson_outline\"][0]\n",
    "\n",
    "    # Generate lesson content (raw text)\n",
    "    lesson_content_text = generate_lesson_content(\n",
    "        lesson_title=first_lesson[\"lesson\"],\n",
    "        learning_objectives=first_unit[\"learning_objectives\"],\n",
    "        lesson_outline=[first_lesson]\n",
    "    )\n",
    "    if lesson_content_text:\n",
    "        all_content += lesson_content_text.strip() + \"\\n\"\n",
    "    # Print the content for debugging\n",
    "    # Return just the raw content string\n",
    "    return all_content\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
